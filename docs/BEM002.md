BEM v0.0.1 – Hierarchical No-Regret Structural Learning VM
with Linear Difficulty Game Ladder
(Integrated English ASCII Spec, implementation-oriented)
	0.	Scope and Intent
0.1 Purpose

This document defines BEM v0.0.1 as a self-contained system consisting of:
	1.	A Game Ladder Engine (GLE) that:
1.1 Represents a family of finite games as a task grammar over BaseProblem classes.
1.2 Generates a linear difficulty ladder of tasks over that family.
1.3 Selects tasks via a no-regret contextual bandit (template bandit).
	2.	A Policy VM (PVM) that:
2.1 Implements a finite-state bitwise VM with a mixture-of-experts (MoE) decision rule.
2.2 Uses bandit-style expert selection per task to guarantee no-regret on each game family.
2.3 Runs with per-step cost bounded by a constant independent of training time.
	3.	A Structure and Curriculum Controller (SCC) that:
3.1 Proposes structural patches to experts and to the task grammar.
3.2 Verifies patches via formal verification conditions VC(Δ).
3.3 Accepts patches only when a PoX (proof-of-improvement) score exceeds a threshold.
3.4 Schedules time between acting (ACT), synthesizing patches (SYNTH), and verification (VERIFY) via a meta-bandit.

BEM v0.0.1 is a virtual machine optimized for learning over finite game families:
given a grammar that spans a finite but arbitrarily rich set of bounded-horizon games, BEM converges (in the no-regret sense) toward near-optimal policies and near-optimal structural decompositions for those games.

0.2 Non-goals

This specification does not define:
	1.	Micro-architecture, caches, or binary instruction encoding.
	2.	Floating-point primitives in the trusted core.
	3.	External user-facing APIs (natural language, GUI, RPC).
	4.	Particular real-world domains; games are abstract MDPs or finite extensive-form games with bounded horizon.
	5.	Human feedback interfaces (HITL) or external tools (filesystems, DB, LLMs); those are separate layers.

0.3 Global configuration (frozen parameters)

The following configuration constants are chosen once at initialization and then treated as read-only:
	1.	State and lanes
1.1 N_state_bits      : total number of STATE bits.
1.2 W                 : SIMD lane width (number of parallel lanes).
	2.	Experts and routing
2.1 B                 : number of routing buckets.
2.2 K_exp             : expert slots per bucket.
2.3 N_in_max          : max Boolean inputs per expert.
2.4 N_out_max         : max Boolean outputs per expert.
2.5 G_gate_max        : max Boolean primitive count per expert circuit.
2.6 M_mono_max        : max ANF monomials per output if ANF is used.
2.7 C_expert_max      : cycle bound per expert call.
2.8 C_step_max        : cycle bound per fast path step.
	3.	Memory partitions
3.1 Sizes and base addresses of logical SHARED segments:
CONFIG, BANDIT, TEACH, WORK, ROUTE_META, PATCH_QUEUE, ENV, META_BANDIT.
	4.	Task and template bounds
4.1 T_tasks_max       : max number of task families (game families).
4.2 S_templates_max   : max number of templates in GLE.
4.3 H_episode_max     : global max episode length.
4.4 Depth_op_tree_max : max depth of a template op_tree.
4.5 Size_op_tree_max  : max number of nodes per op_tree.
	5.	Bandit hyperparameters (expert level)
5.1 alpha_bandit      : exploration weight (fixed-point).
5.2 beta_bandit       : prior weight (fixed-point).
5.3 eta_z             : prior update step size.
5.4 l_ref             : reference loss.
5.5 z_min, z_max      : clipping bounds for expert priors.
	6.	Template bandit hyperparameters (teacher level)
6.1 alpha_teach       : exploration weight for template bandit.
6.2 beta_teach        : prior weight for template bandit.
6.3 eta_z_teach       : prior update for template priors.
6.4 R_meta_min, R_meta_max : meta-reward bounds.
	7.	Reward and difficulty
7.1 R_min < R_max     : environment reward bounds.
7.2 Difficulty band thresholds d_0 < d_1 < … < d_K_band−1.
	8.	Structural learning
8.1 K_patch_queue_max : patch queue capacity.
8.2 w_R, w_C, w_S, w_N : weights for regret, cost, safety, novelty in PoX.
8.3 D_threshold       : minimal PoX score required for patch acceptance.
8.4 p_mut             : mutation probability for template evolution.
	9.	Meta-scheduler
9.1 alpha_meta, beta_meta, eta_z_meta : meta-bandit hyperparameters.
9.2 ACT/SYNTH/VERIFY action set and mapping to operations.

All of the above are stored in CONFIG and mirrored in WORK if necessary.
	1.	Mathematical Model

1.1 Game families

A game family G is defined as a set of bounded-horizon games indexed by a finite parameter set Θ:

G = { g(θ) : θ ∈ Θ }

Each game g(θ) is a finite MDP or a finite extensive-form game with:
	1.	A finite state space S(θ).
	2.	A finite action space A(θ) for the agent (BEM).
	3.	A bounded horizon H(θ) ≤ H_episode_max.
	4.	A reward function r_t ∈ [R_min, R_max].
	5.	Possibly other players with fixed or parameterized policies (for game-theoretic primitives).

1.2 Task instances and difficulty ladder

A task instance τ is a tuple:

τ = (id_τ, game_family_id, θ, difficulty_band, horizon, reward_spec)

The difficulty ladder is a total order over tasks such that:
	1.	Each task τ has an estimated difficulty d_est(τ).
	2.	There exist thresholds d_k such that the difficulty band b(τ) satisfies:
d_k ≤ d_est(τ) < d_{k+1} for band k.
	3.	The Game Ladder Engine is configured so that for each family, it can generate sequences of tasks whose expected difficulty increases approximately linearly with an episode counter or curriculum stage.

Formally, for any fixed game family and a ladder schedule L:

L: n ↦ τ_n with difficulty d_est(τ_n)

the design goal is to approximate:

d_est(τ_n) ≈ d_0 + α * n  for some α > 0, up to bounded noise and saturation.

1.3 No-regret objective (per layer)

Three no-regret problems are instantiated:
	1.	Expert layer (Policy VM)
For each game family and routing bucket b, the expert selector is a bandit algorithm over K_exp arms, with per-step losses in [0,1]. The requirement is:
For each bucket b and family f, for horizon T:
E[Regret_b,f(T)] = E[ Σ_{t=1..T} (ℓ_t(chosen) − min_k ℓ_t(k)) ] = o(T)
In practice, UCB-V-style bounds O(√(K_exp T log T)) are targeted.
	2.	Template layer (Game Ladder Engine)
For each domain and difficulty band, the template bandit chooses templates to maximize meta-reward. It should achieve no-regret relative to the best fixed template mixture within that band.
	3.	Meta layer (SCC)
The meta-bandit chooses between ACT / SYNTH / VERIFY to optimize a meta-reward (e.g. long-run PoX yield, regret reduction). It should achieve no-regret relative to the best fixed schedule in hindsight.
	4.	Runtime State and Memory

2.1 STATE and lanes

STATE is a bit-sliced array:

STATE[k] ∈ {0,1}^W for k in [0, N_state_bits)

Lane l sees bit:

state_lane(l)[k] = STATE[k]_l

All lane-wise operations are implemented via bitwise operations on these W-bit words.

2.2 SHARED segments

SHARED is a flat integer/fixed-point memory, partitioned logically into segments:
	1.	CONFIG         : read-only configuration and hyperparameters.
	2.	BANDIT         : expert-level bandit statistics.
	3.	TEACH          : templates, BaseProblems, ladder metadata.
	4.	WORK           : PoX scores, safety counters, global counters.
	5.	ROUTE_META     : routing descriptors for each task family.
	6.	PATCH_QUEUE    : patch descriptors for structural learning.
	7.	ENV            : environment descriptors, parameters, per-family mappings.
	8.	META_BANDIT    : template bandit and meta-bandit statistics.
	9.	TRACE_META     : pointers and counters for logging segments.

The physical layout (addresses) is implementation-dependent but fixed at configuration.

2.3 Identifiers

Identifiers are 32-bit integers u ∈ {0,…,2^32−1} with bit fields:

u = [class(6) | ecc(6) | bucket(10) | local(10)]
	1.	class (6 bits) encodes object type:
0  = expert
1  = task_family
2  = task_template
3  = patch
4  = env_instance
5  = base_problem
6  = cfg_fragment
others reserved
	2.	ecc (6 bits) is a small ECC or parity code used to spot corruption.
	3.	bucket (10 bits) is meaningful for experts (routing bucket) and can be unused or repurposed for other classes.
	4.	local (10 bits) is a per-class subindex.

Allocators in SHARED ensure:
	•	(class, bucket, local) is unique at allocation time.
	•	ecc field is consistent with (class, bucket, local).

	3.	Game Ladder Engine (GLE)

3.1 BaseProblem families

TEACH defines a fixed set of BaseProblem families that serve as primitive games. Each BaseProblem B_j is:

B_j = (
base_id,        // U, class=base_problem
env_class_id,   // small int, identifies simulator implementation
param_schema,   // param ranges and distributions
horizon_bound,  // <= H_episode_max
complexity_est  // fixed-point
)

A conforming implementation must provide at least these families:
	1.	Bitwise aggregation
1.1 Parity on bit strings of bounded length.
1.2 Majority / threshold.
1.3 Popcount within fixed width.
	2.	Stack and context-free
2.1 Balanced parentheses (Dyck-like).
2.2 Bounded stack machine.
	3.	Key-value and memory
3.1 Key-value retrieval with distractors.
3.2 Limited associative recall.
	4.	Bandit primitives
4.1 K-armed Bernoulli bandit.
4.2 Contextual bandit with simple binary context.
4.3 Non-stationary bandit with a finite number of change points.
	5.	Game-theoretic primitives
5.1 2×2 matrix games vs fixed opponent policies.
5.2 Simple signaling games.
5.3 Single-item auctions with fixed opponent distributions.
	6.	SAT / safety
6.1 Small SAT instances over bounded variable counts.
6.2 Simple safety constraint satisfaction (e.g. avoid BAD region).
	7.	Algorithmic primitives
7.1 Small integer arithmetic, comparisons.
7.2 Small sorting networks.
7.3 Bounded distance computations.
	8.	Token-sequence modeling
8.1 Next-token prediction with finite vocab.
8.2 Simple transducer emulation.
8.3 Copy / reverse / permute tasks.

For each B_j, env_class_id maps to a simulator implementation in ENV; param_schema defines, for example, ranges for length, K, payoffs.

3.2 Task templates

A template t is stored as:

TEMPLATE[t_id] = (
id_t,          // U, class=task_template
domain_id,     // small int
op_tree_root,  // index into a node array
param_schema,  // parameter sampling rules
d_est,         // difficulty estimate
stats          // template-level statistics
)

stats:

stats = (
usage_count,    // u64
pass_rate,      // fixed-point
regret_proxy,   // fixed-point
novelty,        // fixed-point
last_update_step // u64
)

3.3 op_tree representation

op_tree is represented as an array of nodes:

NODE[i] = (
op_type,        // enum
child_idx_0,    // int or -1
child_idx_1,    // int or -1
local_params    // small fixed vector
)

op_type ∈ {
LEAF_BASE,      // BaseProblem leaf
LEAF_ISA,       // CFG fragment leaf
SEQ,
PAR,
NEST,
MASK,
INTERLEAVE,
REPEAT,
BRANCH,
LOOP
}

Constraints:
	1.	Tree depth ≤ Depth_op_tree_max.
	2.	Node count ≤ Size_op_tree_max.
	3.	All leaves are either BaseProblems or CFG fragments.

3.4 Template instantiation to task instance

Algorithm TEACH_INSTANTIATE(t_id):

Input: template t_id, random seed s.
Output: task instance τ and env_config.

Steps:
	1.	Read TEMPLATE[t_id].param_schema and sample θ following specified distributions, using PRNG seeded with s and t_id.
	2.	Traverse op_tree from root, constructing an environment skeleton:
2.1 LEAF_BASE: instantiate BaseProblem B_j with params θ_j derived from θ.
2.2 LEAF_ISA: instantiate ISA-level fragment with parameters.
2.3 SEQ, PAR, NEST, MASK, INTERLEAVE, REPEAT, BRANCH, LOOP: create composition nodes according to operator semantics, wiring sub-envs as subgraphs.
	3.	From the skeleton, compute:
3.1 game_family_id (or task_family_id).
3.2 horizon H(τ) ≤ H_episode_max.
3.3 difficulty estimate d_est(τ) (local estimate from structure and param ranges).
3.4 reward_spec describing reward scaling and combination across sub-envs.
	4.	Choose difficulty_band b(τ) according to threshold table and d_est(τ).
	5.	Allocate id_τ (class=task_instance) if needed.
	6.	Write env_config (env_id, parameters, composition descriptor) into ENV.
	7.	Return τ = (id_τ, game_family_id, θ, b(τ), H(τ), reward_spec).

Complexity: bounded by a constant that depends on op_tree bounds, not on training time.

3.5 Template bandit and ladder

For each domain d and difficulty band k, TEACH maintains bandit stats over templates t in that (d, k) bucket:
	1.	meta_n[d][k][t]     : u32 selection count.
	2.	meta_L[d][k][t]     : fixed-point sum of meta_losses.
	3.	meta_Q[d][k][t]     : fixed-point sum of meta_loss^2.
	4.	meta_z[d][k][t]     : prior log-weight.

Meta-reward and loss:

meta_reward_t = f(pass_rate_t, regret_proxy_t, novelty_t)
meta_loss_t   = (R_meta_max − meta_reward_t) / (R_meta_max − R_meta_min)

TEACH_CHOOSE(d, k):
	1.	For candidate templates t in domain d, band k:
1.1 n = max(1, meta_n[d][k][t])
1.2 L = meta_L[d][k][t], Q = meta_Q[d][k][t]
1.3 hat_l = L / n
1.4 var = max(0, Q/n − hat_l^2)
1.5 n_tot = max(1, Σ_t meta_n[d][k][t])
1.6 bonus = alpha_teach * sqrt( (2 * var * log(1 + n_tot)) / n )
+ alpha_teach * (3 * log(1 + n_tot)) / n
1.7 prior_term = −beta_teach * meta_z[d][k][t]
1.8 index_t = hat_l − prior_term + bonus
	2.	Choose template t* with minimal index_t (tie-break by smallest id_t).
	3.	Return t*.

Difficulty ladder behavior:

By controlling bands and mapping from curriculum state (e.g. competence metrics) to requested band k, GLE ensures that sampled tasks follow a roughly linear increase in difficulty. The ladder controller is a deterministic mapping:

curriculum_stage n → (d, k(n))

with k(n) monotone non-decreasing and tasks produced by TEACH_CHOOSE(d, k(n)).

3.6 Template mutation and expansion

With probability p_mut per selection of template t, TEACH generates a mutated template t_new:
	1.	Select a small local transformation:
1.1 Insert MASK node around a leaf or subtree.
1.2 Replace a LEAF_BASE by a SEQ of two leaves.
1.3 Increase loop bounds within safe ranges.
1.4 Interleave another BaseProblem.
	2.	Ensure resulting op_tree obeys depth and size bounds.
	3.	Compute initial d_est(t_new) from structure and parental d_est(t).
	4.	Initialize bandit stats meta_n, meta_L, meta_Q, meta_z for t_new using neutral priors or small perturbation from t.
	5.	Insert t_new into TEACH storage.
	6.	Policy VM (PVM)

4.1 Expert table and buckets

For each bucket b ∈ {0,…,B−1}, there is a fixed array of expert slots:

EXPERT[b][k] for k in {0,…,K_exp−1}

Each slot is:

EXPERT[b][k] = (
expert_id,   // U, class=expert, bucket=b
R_spec,      // read spec
W_spec,      // write spec
C_rep,       // circuit representation
profile      // static cost/profile
)

R_spec:
	1.	state_bits[0..R_state_max−1]    : STATE indices.
	2.	shared_words[0..R_shared_max−1] : SHARED word indices.

W_spec:
	1.	state_bits[0..W_state_max−1]    : STATE indices to write.
	2.	shared_words[0..W_shared_max−1] : SHARED indices to write.
	3.	mapping from circuit outputs to these targets.
	4.	configuration of lane masking bits.

C_rep:
	1.	Boolean circuit over N_in Boolean inputs, where N_in ≤ N_in_max.
	2.	Optional integer outputs computed as fixed linear combinations or MACs.
	3.	Total gate count ≤ G_gate_max, monomials ≤ M_mono_max if ANF.

profile:
	1.	cost_expert[b][k]   : static upper bound on cycles.
	2.	usage counters for monitoring.

4.2 Routing descriptor

For each task family τ_f there is:

ROUTE_DESC[τ_f] = (
state_bit_indices[],   // indices into STATE
shared_word_indices[], // indices into SHARED
hash_params            // constants for hash function
)

Routing signature at step t for family τ:

SIG(STATE_t, SHARED_t, τ) = k_t ∈ Z_64

computed via:
	1.	Load specified STATE and SHARED entries.
	2.	Combine them via fixed XOR, shifts, multiplies.
	3.	Return 64-bit integer k_t.

Bucket index:

b_t = k_t mod B

4.3 Expert bandit state

For each (b, k, τ_f):
	1.	n[b][k][τ_f] ∈ u32   : visit count.
	2.	L[b][k][τ_f] ∈ FP    : sum of normalized losses.
	3.	Q[b][k][τ_f] ∈ FP    : sum of squared losses.
	4.	z[b][k][τ_f] ∈ FP    : prior log-weight.

For each τ_f:

N_τ[τ_f] ∈ u64          : total selections.

4.4 Loss normalization

Given reward r_t ∈ [R_min, R_max], normalized loss is:

loss_t = (R_max − r_t) / (R_max − R_min) ∈ [0,1]

4.5 Expert selection (BANDIT_CHOOSE)

At step t, given task family τ_f and bucket b_t:
	1.	For k in 0..K_exp−1:
1.1 n = max(1, n[b_t][k][τ_f])
1.2 L = L[b_t][k][τ_f], Q = Q[b_t][k][τ_f]
1.3 hat_l = L / n
1.4 var = max(0, Q/n − hat_l^2)
1.5 n_tot = max(1, N_τ[τ_f])
1.6 bonus = alpha_bandit * sqrt( (2 * var * log(1 + n_tot)) / n )
+ alpha_bandit * (3 * log(1 + n_tot)) / n
1.7 prior_term = −beta_bandit * z[b_t][k][τ_f]
1.8 index[b_t][k][τ_f] = hat_l − prior_term + bonus
	2.	Choose k_t as argmin index[b_t][k][τ_f].
	3.	Return k_t.

4.6 Expert execution

Given (b_t, k_t) and current (STATE_t, SHARED_t):
	1.	Input extraction:
1.1 Read STATE bits and SHARED words as per R_spec.
1.2 Pack them into Boolean vector x.
	2.	Circuit evaluation:
2.1 Evaluate C_rep deterministically using BIT-ALU and integer ALU.
2.2 Produce Boolean outputs y and integer outputs z_int.
	3.	Lane mask:
3.1 Derive mask_lane ∈ {0,1}^W from designated bits of y or from z_int.
3.2 mask_lane[l] = 1 means lane l is active for write.
	4.	Write-back:
4.1 For STATE targets:
STATE[k] := (STATE[k] & ~mask_lane) | (y_bits & mask_lane)
for appropriate output bits.
4.2 For SHARED targets:
Update SHARED words from z_int and constants.
	5.	Cost constraint:
The circuit plus R_spec and W_spec must be implementable within C_expert_max cycles on the target ISA.

4.7 Bandit update (BANDIT_UPDATE_STEP)

When reward r_t is available for step t:
	1.	Compute loss_t from r_t.
	2.	Let (b_t, k_t, τ_f) be recorded for this step.
	3.	Update:
n_old = n[b_t][k_t][τ_f]
L_old = L[b_t][k_t][τ_f]
Q_old = Q[b_t][k_t][τ_f]
N_τ_old = N_τ[τ_f]
n_new     = n_old + 1
N_τ_new   = N_τ_old + 1
L_new     = L_old + loss_t
Q_new     = Q_old + loss_t * loss_t
n[b_t][k_t][τ_f] = n_new
L[b_t][k_t][τ_f] = L_new
Q[b_t][k_t][τ_f] = Q_new
N_τ[τ_f]         = N_τ_new
	4.	Prior update:
avg_l  = L_new / max(1, n_new)
delta_z = −eta_z * (avg_l − l_ref)
z_temp  = z[b_t][k_t][τ_f] + delta_z
z[b_t][k_t][τ_f] = clip(z_temp, z_min, z_max)

All operations use fixed-point arithmetic as defined in CONFIG.

4.8 Environment interface

For each environment instance, there is an env driver defined in ENV that:
	1.	Reads selected action bits or words from STATE/SHARED.
	2.	Updates its internal simulator state.
	3.	Writes reward r_t and done flag into SHARED.
	4.	Optionally writes observation bits into STATE for next step.

PVM treats the environment as a deterministic or stochastic mapping:

(s_env, action_t) → (s_env’, reward_t, done_t)

and only interacts via STATE/SHARED.
	5.	Structure and Curriculum Controller (SCC)

5.1 Patch descriptors

PATCH_QUEUE is a circular array of descriptors:

PATCH[i] = (
patch_id,         // U, class=patch
kind,             // enum
target,           // id(s) of affected objects
payload_ptr,      // pointer to patch-specific data
payload_len,      // length
effects_est,      // estimated Δs
status            // PENDING, VERIFIED, REJECTED, APPLIED
)

kind ∈ {
RETIRE,
RESET,
SPLIT,
MERGE,
SUPEROPT,
TEACH_EDIT
}

effects_est:

effects_est = (
delta_regret_est,   // FP
delta_cost_est,     // FP
delta_safety_est,   // FP
delta_novelty_est   // FP
)

5.2 Patch kinds (semantics)

RETIRE:
	•	Replace EXPERT[b][k] with NOOP expert.
	•	Reset bandit stats to neutral.
	•	Used for consistently bad experts.

RESET:
	•	Reset bandit stats for EXPERT[b][k] without changing its program.
	•	Used when environment distribution changes or for exploration.

SPLIT:
	•	Given EXPERT[b][k], create new slot k_new in same bucket and add gating bit g*.
	•	Derive two experts specialized on g* = 0 and g* = 1 region.
	•	Update ROUTE_DESC to include gating predicate.
	•	Initialize bandit stats for new slot from parent.

MERGE:
	•	Given cluster {(b,k_1),…,(b,k_m)}, synthesize a merged expert via CEGIS, then RETIRE old ones and install new merged expert.

SUPEROPT:
	•	Optimize C_rep of EXPERT[b][k] locally (reduce cost_expert) while preserving semantics up to tested domain.
	•	Verified via equivalence checks on representative input sets.

TEACH_EDIT:
	•	Add or modify a TEACH template, including BaseProblem composition and param_schema.
	•	Must respect safety and complexity bounds.

5.3 Verification kernel and VC(Δ)

Each patch Δ has a verification condition VC(Δ) represented as CNF and checked by a SAT/CEGIS engine:
	1.	RETIRE / RESET:
	•	Indices in range.
	•	NOOP expert obeys R_spec/W_spec constraints and cost bounds.
	2.	SPLIT:
	•	Gating bit index is allowable.
	•	New expert descriptors respect bounds.
	•	routing invariants (fixed candidate set size, etc.) are preserved.
	3.	MERGE:
	•	Input/output behavior on test set matches.
	•	Gate/monomial bounds respected.
	4.	SUPEROPT:
	•	New C_rep reduces cost_expert.
	•	Equivalence to old C_rep on test domain.
	5.	TEACH_EDIT:
	•	Template horizon bound ≤ H_episode_max.
	•	op_tree size and depth within limits.
	•	Safety tags preserved or strenghtened.
	•	Domain and band assignments consistent.

SAT_CHECK(VC(Δ)) yields SAT, UNSAT, or UNKNOWN.
	•	UNSAT: no violation; VC(Δ) holds.
	•	SAT or UNKNOWN: patch is rejected.

5.4 PoX scoring

For VERIFIED patches (VC(Δ) UNSAT), PoX computes:

score(Δ) = w_R * delta_regret + w_C * delta_cost
+ w_S * delta_safety + w_N * delta_novelty

where:
	•	delta_regret     : estimated improvement in regret.
	•	delta_cost       : estimated improvement in cycle cost.
	•	delta_safety     : improvement in safety metrics.
	•	delta_novelty    : coverage / diversity gain.

Acceptance:
	•	If score(Δ) ≥ D_threshold => Δ eligible for application.
	•	Else Δ is set to REJECTED.

5.5 Patch application

Patch application is atomic with respect to:
	1.	EXPERT entries and ROUTE_DESC entries modified.
	2.	TEACH templates modified.
	3.	BANDIT and TEACH bandit stats initializations for new structures.
	4.	Merkle roots for EXPERT, TEACH, WORK.
	5.	TRACE log entry describing patch application.

Fast path sees either pre-patch or post-patch state, never an intermediate.

5.6 Meta-bandit (ACT / SYNTH / VERIFY)

SCC defines a meta-bandit over three actions:
	1.	ACT    : run episodes, collect data, perform BANDIT_UPDATE.
	2.	SYNTH  : generate patches based on TRACE and statistics.
	3.	VERIFY : evaluate VC(Δ), compute PoX, apply accepted patches.

Meta-bandit state:

For each meta action a ∈ {ACT, SYNTH, VERIFY}:
	•	meta_n_meta[a]     : selection count.
	•	meta_L_meta[a]     : sum of meta_loss.
	•	meta_Q_meta[a]     : sum of meta_loss^2.
	•	meta_z_meta[a]     : prior log-weight.

Meta reward is defined from:
	•	PoX yield (applied patches per unit verify time).
	•	regret reduction rates (delta_regret over windows).
	•	safety metrics.

Meta_bandit uses the same UCB-V + prior index form as expert bandit and template bandit.
	6.	Execution Model

6.1 Fast path step (STEP_FAST)

For each environment instance and step t:
	1.	Load current task family τ_f from SHARED.
	2.	Compute routing signature:
k_t = SIG(STATE_t, SHARED_t, τ_f)
b_t = k_t mod B
	3.	Expert selection:
k_t = BANDIT_CHOOSE(τ_f, b_t)
	4.	Expert execution:
Execute EXPERT[b_t][k_t] to update (STATE_t, SHARED_t) → (STATE_t’, SHARED_t’).
	5.	Environment update:
Environment driver reads action from STATE/SHARED, updates env state, writes reward r_t and done flag.
	6.	Logging (light):
Append e_t = (task_id_t, b_t, k_t, reward_proxy, cost_proxy) to TRACE ring buffer or log segment.
Update incremental hash chain H.
	7.	Defer BANDIT_UPDATE_STEP to mid-path (or perform it if immediate reward and cost allow and C_step_max is not exceeded).

Constraint:
Total cycles for steps 1–6 must not exceed C_step_max.

6.2 Mid-path

Mid-path routines operate periodically:
	1.	BANDIT_UPDATE_STEP for all steps with reward now available.
	2.	Aggregate per-task and per-template statistics from TRACE.
	3.	TEACH_BANDIT updates (meta_n/meta_L/meta_Q/meta_z) for templates.
	4.	Update d_est(t) and difficulty bands for templates.
	5.	SCC maintenance:
5.1 Evaluate windows of regret, cost and safety to update effects_est for candidate patches.
5.2 Manage PATCH_QUEUE statuses (e.g., mark obsolete patches REJECTED).

All mid-path routines have cost bounded per invocation, independent of total training time, via window sizes and capped loops.

6.3 Slow-path

Slow-path is invoked by meta-bandit’s SYNTH and VERIFY actions:
	1.	SYNTH:
1.1 Analyze TRACE and BANDIT/TEACH stats to propose patches.
1.2 Insert patches into PATCH_QUEUE as PENDING.
	2.	VERIFY:
2.1 For selected PENDING patches, run VC(Δ) via verification engine.
2.2 Compute PoX score and update status to VERIFIED, APPLIED, or REJECTED.
2.3 Apply APPLIED patches atomically.
2.4 Optionally trigger snapshots.

Slow-path does not have per-step cost constraints; it is scheduled by meta-bandit in a way that preserves fast path guarantees.
	7.	Logging, Integrity, Snapshot

7.1 TRACE

TRACE is logically an append-only log. Implementation can use ring buffers plus periodic offload.

Entry types:
	1.	Step entries:
(step_id, task_id, bucket_index, expert_slot, reward_proxy, cost_proxy)
	2.	Episode summaries:
(episode_id, task_id, return, length, regret_estimate)
	3.	Patch events:
(patch_id, kind, target, status, score, time)
	4.	Snapshot markers and Merkle roots.

7.2 Hash chain

H_0 is a fixed constant.
For each appended entry e_i:

H_{i+1} = Hash(H_i || encode(e_i))

The current H is stored in WORK and can be exported for audit.

7.3 Merkle trees

Merkle trees are maintained separately for:
	1.	EXPERT table.
	2.	TEACH templates.
	3.	WORK config/hyperparameters.

Each APPLIED patch updates relevant subtrees and Merkle roots. Roots are logged in TRACE.

7.4 Snapshot

A snapshot includes at minimum:
	1.	STATE and SHARED.
	2.	EXPERT and ROUTE_DESC.
	3.	TEACH templates and bandit state.
	4.	BANDIT and META_BANDIT states.
	5.	PATCH_QUEUE state.
	6.	TRACE head pointer and hash chain head.
	7.	Merkle roots.

Restoring a snapshot re-creates an equivalent BEM instance with identical future behavior given the same environment stochasticity.
	8.	Guarantees and Design Goals

8.1 Computational guarantees
	1.	Per-step cost of fast path is bounded by C_step_max.
	2.	All bandit operations (expert, template, meta) are O(K) per decision, with small fixed K_exp, S_templates_per_band, and 3 meta actions.
	3.	Patches and template updates are applied only through bounded-cost slow-path routines.

8.2 Learning guarantees (informal)

Expert level:
	•	For each (b, τ_f), normalized loss in [0,1] and UCB-V-style index with bounded priors gives sublinear regret in T:
E[Regret_b,τ_f(T)] = O(√(K_exp T log T)).

Template level:
	•	With same assumptions over meta_loss_t, TEACH_BANDIT achieves similar no-regret guarantees over templates in each (domain, band).

Meta level:
	•	The ACT/SYNTH/VERIFY meta-bandit similarly satisfies a no-regret bound against fixed schedules, under bounded meta_rewards.

8.3 Closure over finite games

Given:
	1.	A finite game family G with bounded horizon and state/action spaces.
	2.	A TEACH grammar and BaseProblem set satisfying the closure property (any such game can be represented or approximated by some template composition).

Then there exists a template set and mapping in TEACH such that, up to approximation error, the Game Ladder Engine can represent G, and the Policy VM can learn near-optimal expert mixtures under no-regret.

This is a design requirement, not proven here, but the grammar and BaseProblem requirements are chosen to make it realistic.

8.4 Safety and monotonic structural improvement

Because every patch must satisfy VC(Δ) and PoX score(Δ) ≥ D_threshold, the system is constrained to monotone structural changes with respect to the chosen metrics:
	•	Structural changes must not violate explicit safety invariants.
	•	Structural changes must pass proof-of-improvement filters, making regressions rare in expectation.

	9.	Initial Conditions

9.1 Experts

At boot:
	1.	EXPERT table is initialized with:
1.1 Identity-like experts.
1.2 Simple bit-flip and permute experts.
1.3 Counter experts in SHARED.
1.4 NOOP experts.
	2.	BANDIT stats initialized to neutral (e.g. n = 1, L = l_ref, z = 0).

9.2 Templates and ladder

At boot:
	1.	minimal set of TEACH templates over BaseProblems with short horizons and diverse but simple tasks.
	2.	initial difficulty ladder is coarse (few bands).
	3.	as learning progresses, TEACH mutates and adds templates to tighten the ladder around agent competence.

9.3 Determinism

Given:
	1.	fixed configuration and code,
	2.	fixed PRNG seeds,
	3.	deterministic environment or fixed random seeds for environments,

BEM v0.0.1 is deterministic: same inputs yield the same sequence of tasks, actions, patches, and snapshots.

⸻

This spec reorganizes BEM as:
	•	Game Ladder Engine = linear difficulty task grammar and template bandit.
	•	Policy VM = finite-state MoE VM with expert-level no-regret RL.
	•	Structure and Curriculum Controller = patch generator + verifier + PoX-gated structural learner + meta-bandit.

All parts are defined so that an implementation can be written directly from this document without relying on external naming or previous drafts.
