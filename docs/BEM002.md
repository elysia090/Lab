BEM v0.0.1 – Boolean Expert Machine
Hardware-Oriented Core, ISA, Algorithms, and No-Regret Teacher
(Integrated English ASCII Draft, no separators)
	0.	Scope and Goals

0.1 Purpose

This document defines the Boolean Expert Machine (BEM) v0.0.1 as a hardware-oriented, finite-state abstract machine with:
	1.	A single integer control core plus a fixed set of co-processors.
	2.	A fast path that uses only integer arithmetic and bitwise operations, with bounded per-step cost independent of episode length and total steps.
	3.	A family of Boolean experts acting as local state transformers over bit-sliced state.
	4.	Online learning via contextual bandits with no-regret guarantees (UCB-style) implemented entirely on the fast path in fixed-point arithmetic.
	5.	Mid-path GRPO-like policy updates that use episode returns but preserve bounded fast-path cost.
	6.	Structural updates (expert split/merge, macro extraction, circuit rewrites) subject to formal verification via a PROVER kernel.
	7.	An internal teacher that automatically generates and schedules composite tasks, without requiring hand-written primitive tasks, using no-regret curriculum bandits.
	8.	Interfaces for self-play, synthetic experiments, and counterexample-guided refinement.
	9.	A minimal RISC ISA and explicit algorithms for routing, learning, scheduling, verification, and teacher operation.

The design is intended so that, once instantiated, BEM can be run in a largely automatic regime: the fast path runs a theoretically grounded bandit RL loop, the teacher generates and composes tasks, and structural changes are gated by proofs and a proof-of-improvement (PoX) objective.

0.2 Non-goals

BEM v0.0.1 does not define:
	1.	Concrete binary encodings or micro-architectural details.
	2.	Concrete performance targets or power envelopes.
	3.	Any host programming language, ABI, or OS integration.
	4.	Floating-point primitives or neural networks on the fast path.

Neural or floating-point components may exist outside the core (for example in ANN, PROVER, TEACHER, or domain evaluators) as untrusted proposal or oracle modules. All proposals must be validated through the verification kernel and PoX logic before affecting the trusted configuration.

0.3 Notation and Complexity

Bits are elements of {0,1}. Bitvectors of length n are elements of {0,1}^n. Integers are elements of Z with fixed-width representations.

Per-step complexity refers to control-core cycles and co-processor invocations triggered by one logical step (event or token). Fast-path cost must be bounded by a constant independent of:
	•	Episode length.
	•	Total number of past steps.

Global configuration parameters:
	•	N: number of global state bits.
	•	K: number of shared memory bits (K >= N).
	•	P: number of fixed-point parameters.
	•	W: SIMD width (lanes).
	•	S: maximum number of experts (slots in EXPERT table).
	•	d: dimension of feature bits for routing (for example 64 to 256).
	•	k_large: ANN candidate size (for example 16 to 64).
	•	k_small: final routing candidate size (for example 1 to 4).
	•	D_max: maximum graph search depth inside ANN.
	•	M_max: maximum neighbor degree in ANN.
	•	G: GRPO-lite group size (for example 4 or 8).
	•	d_ctx: context feature dimension for schedulers.
	•	D_task: maximum depth for teacher-generated composite tasks.

All are fixed at design or configuration time.
	1.	Machine State and Identifiers

1.1 Logical State

The logical state at time t is:

X_t = (s_t, M_t, Theta_t)

where:
	•	s_t in {0,1}^N: global state bits (bit-sliced over W lanes physically).
	•	M_t in {0,1}^K: shared memory bits.
	•	Theta_t in Z^P: fixed-point parameters (bandit weights, scheduler parameters, teacher parameters, etc).

Fast path reads and writes s_t and M_t and may read Theta_t. Structural update, teacher, and verification may modify Theta_t and structural tables.

1.2 Configuration

Structural configuration C includes:
	•	EXPERT table (experts, circuits, bandit stats, routing metadata).
	•	CFG (instruction array and CFG metadata).
	•	ANN index and routing metadata.
	•	TEACHER configuration (task grammar, curriculum stats).
	•	INVAR and PROOF configuration.
	•	WORK configuration (PoX weights and scheduler parameters).
	•	LAYOUT and ID maps.

Fast path may read C but must not modify it directly. All structural and teacher changes proceed via patches.

1.3 Identifier Space

Domain U = {0, 1, …, 2^32 - 1}. Each identifier u in U is structured:

u = [class(6) | ecc(6) | shard(6) | local(14)]

where:
	•	class in {0..63}: object class (expert, cfg_node, variable, template, hypothesis, task_template, etc).
	•	ecc in {0..63}: parity or ECC bits for shard and local.
	•	shard in {0..63}: logical shard index.
	•	local in {0..16383}: shard-local index.

Gray encoding:

g(u) = u xor (u >> 1)

Hamming distance d_H(a, b) is the number of differing bits. A simple similarity is:

sim(u, v) = 32 - d_H(g(u), g(v))

1.4 Slot Mapping

Objects with tabular storage (experts, macros) use a slot function:

slot: U -> {0..S-1} union {bottom}

For allocated expert identifier u:
	•	i = slot(u) in {0..S-1}.
	•	EXPERT[i] is descriptor for u.

Requirements:
	1.	slot(u) != bottom for any allocated expert u.
	2.	If u != v and both are allocated experts, then slot(u) != slot(v).
	3.	slot mapping and its inverse are stored in SHARED.
	4.	Rebalancing may change slot(u) but must preserve:
	•	Object content.
	•	Semantics of references that use identifiers u.

1.5 Task and Context Identifiers

Per-step metadata:
	•	task_id_t in Z_small: integer distinguishing task families or teacher-generated task classes.
	•	context_hash_t in Z_32 or Z_64: rolling hash summarizing recent observations, rewards, and actions.

These reside in STATE or SHARED and are computed by deterministic functions:

task_id_t = T_id(s_t, M_t, external_task_config, teacher_task_desc_t)
context_hash_t = H_ctx(context_hash_{t-1}, obs_{t-1}, r_{t-1}, a_{t-1})

H_ctx uses integer arithmetic and bitwise operations only.
	2.	Memory Segments

2.1 Logical Segments
	1.	STATE
	•	Holds bit-sliced representation of s_t over W lanes.
	•	STATE[k] in {0,1}^W for k in {0..N-1}.
	2.	SHARED
	•	Holds M_t, Theta_t, configuration, routing metadata, bandit and GRPO stats, scheduler statistics, ANN descriptors, teacher descriptors, curriculum statistics, etc.
	3.	EXPERT
	•	Entries EXPERT[0..S-1] with expert descriptors, circuits, stats.
	4.	CFG
	•	Instruction array C[0..L-1], CFG node metadata, macro descriptors.
	5.	TRACE
	•	Step-level traces, episode summaries, patch metadata, PoX entries.
	6.	PROOF
	•	CNF formulas, solver contexts, proof objects, unsat cores, CEGIS data.
	7.	WORK
	•	PoX configuration, difficulty, moving averages, scheduler matrices and vectors.
	8.	TEACH
	•	Task grammar, generator parameters, per-template stats, curriculum bandit state.

Segments are logically disjoint. Physical mapping is implementation-specific but must preserve semantic isolation.

2.2 STATE Segment Representation

For each bit index k in [0, N):
	•	S_pl[k] in {0,1}^W holds bit k for all lanes.

Lane l view of s_t:

s_t^(l)[k] = bit l of S_pl[k]

Shared memory M_t and parameters Theta_t are not bit-sliced.

2.3 EXPERT Entry Format

For each slot i:

EXPERT[i] = (id_i, R_spec_i, W_spec_i, C_rep_i, bandit_i, routing_meta_i)

where:
	•	id_i in U: expert identifier.
	•	R_spec_i: input selection specification.
	•	W_spec_i: write-back specification.
	•	C_rep_i: Boolean circuit representation.
	•	bandit_i includes per-task bandit state:
	•	For each task tau in a small index set:
	•	visits_i_tau: 32-bit integer.
	•	loss_sum_i_tau: fixed-point.
	•	loss_sq_sum_i_tau: fixed-point.
	•	z_i_tau: fixed-point log-weight.
	•	last_update_step_i_tau: 64-bit step counter.
	•	routing_meta_i:
	•	priority tags, macro tags, version, index metadata.

2.4 CFG Segment
	•	C[pc] is an abstract instruction executed by BEM-CORE.
	•	CFG nodes v in U (class cfg_node) include:
	•	pc_start[v], pc_end[v].
	•	Succ(v): successor node ids.
	•	Optional local invariants P_v (CNF in PROOF).

Macros:
	•	Represent common expert sequences or hot CFG fragments.
	•	Macro nodes reference subprograms or expert sequences.

	3.	Hardware Modules

3.1 BEM-CORE

BEM-CORE executes the ISA:
	•	Integer arithmetic and bitwise operations.
	•	Control flow.
	•	Scalar loads and stores.
	•	Co-processor commands.

Responsibilities:
	•	Fast-path loop control (STEP_FAST).
	•	Bandit updates (Algorithm B) in fixed-point.
	•	Mid-path GRPO updates.
	•	Scheduling of slow-path tasks (verification, structural updates, teacher updates).

3.2 BIT-ALU

BIT-ALU operates on W-bit bitvectors:
	•	AND, OR, XOR, NOT.
	•	Shifts and rotates.
	•	POPCOUNT and parity.
	•	Mask-based blends.

Latency of BIT-ALU operations is bounded by a small constant.

3.3 ANN Unit

Interface:

ANN_QUERY(desc_ptr) -> candidate_list_ptr

Descriptor in SHARED:
	•	q: 32-bit key.
	•	f: feature bitvector of length d.
	•	k: desired number of candidates (<= k_large).
	•	config: routing configuration for task.

Output list:
	•	count (<= k).
	•	Sequence of slot indices or identifiers.

Internal constraints:
	•	Search depth <= D_max.
	•	Per-node degree <= M_max.
	•	Worst-case query cost bounded by constant given D_max and M_max.

3.4 SAT and CEGIS Unit (PROVER)

Interfaces:
	•	SAT_CHECK(cnf_id, options) -> SAT, UNSAT, UNKNOWN plus optional unsat_core_id.
	•	PROOF_CHECK(cnf_id, proof_id) -> ACCEPT or REJECT.
	•	HOARE_CHECK(cfg_id, annotations_id) -> ACCEPT or REJECT.
	•	CEGIS(phi_id, hyp_class_id, options) -> candidate_id or NONE, counterexample_id or NONE.

Supports:
	•	Incremental CNF with base contexts.
	•	Unsat core extraction.
	•	Multiple solver profiles and circuit-aware heuristics chosen by BEM-CORE.

3.5 HASH and ECC Unit

Responsibilities:
	•	ECC encode and decode for identifiers and memory blocks.
	•	Hash computation Hash: {0,1}* -> {0,1}^h (for example h = 256).
	•	Merkle tree updates.
	•	Hash chains: H_{k+1} = Hash(H_k || entry_k).

3.6 LOG Unit

Responsibilities:
	•	Append structured entries to TRACE.
	•	Maintain hash chain H_k in protected state.
	•	Maintain counters and moving averages for PoX and drift metrics.

	4.	Observation and Routing Inputs

4.1 Observation Function

Observation is a deterministic function:

obs_t = O(s_t, M_t) = (task_id_t, context_hash_t, local_bits_t, feature_bits_t)

Components:
	•	task_id_t:
	•	Stable for the duration of a teacher-selected task instance.
	•	Derived from configuration, teacher_task_desc_t, and environment.
	•	context_hash_t:
	•	32- or 64-bit rolling hash:
context_hash_t = H_ctx(context_hash_{t-1}, small summary of obs_{t-1}, r_{t-1}, a_{t-1})
	•	Implemented via integer multiply-add and xor.
	•	local_bits_t:
	•	Fixed subset of bits from s_t and M_t, configured per task family.
	•	feature_bits_t in {0,1}^d:
	•	Derived from F_feat(s_t, M_t, context_hash_t) via random projections and thresholds or sketches.

4.2 Query Key

From obs_t:

q_t = F_id(task_id_t, context_hash_t)

Example:

q_t = low32(Hash256(task_id_t || context_hash_t))

F_id must distribute contexts across shards using integer and bit operations only.

4.3 Shards and Buckets

ANN Unit derives:
	•	shard_id = high bits or Gray-coded prefix of q_t.
	•	bucket_id = selected bits of q_t.

Index structure:

index[shard_id][bucket_id] = small set of candidate experts plus local ANN graph or cache.
	5.	Expert Semantics and Circuits

5.1 Expert as Local Transformer

Given global state (s, M), expert i defines:

x = R_i(s, M) in {0,1}^{n_i}
y = C_i(x) in {0,1}^{m_i}
(s’, M’) = W_i(s, M, y)

where:
	•	R_i is determined by R_spec_i:
	•	Bit indices into s and M.
	•	Simple address expressions.
	•	C_i is given by C_rep_i:
	•	Either ANF or ROBDD-like form.
	•	W_i is determined by W_spec_i:
	•	Write-back to specific bits or contiguous regions.
	•	Optional lane masks derived from y.

5.2 Circuit Representation

ANF form:
	•	Each output y_j is xor of monomials, each monomial an AND of subset of inputs.

ROBDD form:
	•	DAG with nodes representing tests on specific input bits.
	•	Canonical variable ordering.
	•	Reduced graph.

Common requirements:
	•	Circuits must support bit-sliced evaluation:
	•	Inputs x for all lanes are packed in W-bit words.
	•	All intermediate values are W-bit.
	•	Operations use BIT-ALU primitives.

5.3 Fast-Path Evaluation Bound

Design-time bounds:
	•	n_i <= N_in_max.
	•	m_i <= N_out_max.
	•	Monomial count per output <= M_mono_max.

Total cost per expert evaluation is bounded:

Cost_expert(i) <= C_expert_max

C_expert_max depends on N_in_max, N_out_max, M_mono_max but not on episode length.
	6.	Fast-Path Control and Routing

6.1 Two-Stage Routing Algorithm

At each logical step t:
	1.	Observation:
	•	obs_t = O(s_t, M_t).
	2.	Query construction:
	•	(q_t, f_t) = (F_id(task_id_t, context_hash_t), feature_bits_t).
	3.	ANN candidate retrieval:
	•	C_large = ANN_QUERY(q_t, f_t, k_large, config(task_id_t)).
	4.	Bandit-based filtering via BANDIT_CORE.CHOOSE:
	•	Given task tau = task_id_t and candidate set C_large, compute bandit scores and select:
	•	Either a single expert i_t, or
	•	A subset C_t of size k_small.
	5.	Expert batch evaluation:
	•	BEM_EXPERT_BATCH(i_t or C_t, STATE, SHARED).
	6.	Minimal stats updates:
	•	Fast-path bookkeeping needed to later apply BANDIT_UPDATE_STEP once reward r_t is known.

Fast path must not call SAT, heavy CEGIS, teacher structural edits, or patch application.

6.2 Lane-Level Assignment

Assignment strategies:
	•	Single expert for all lanes: one i_t applied to each lane independently.
	•	Grouped lanes: lanes partitioned into fixed groups, one expert per group.
	•	Per-lane expert: allowed but must remain bit-sliced with lane masks.

BEM_EXPERT_BATCH applies R_spec, C_rep, W_spec consistently across lanes using bit-masks.

6.3 Fast-Path Complexity Bound

Per step t, fast-path cost satisfies:

Cost_step <= C_obs + C_ANN + C_bandit_choose + C_expert_batch + C_stats

All C_* depend only on configuration constants, not on episode length or total steps.
	7.	Bandit Core v0.0.1 (No-Regret Fast-Path RL)

7.1 Data Structures

For each expert i and task tau:
	•	visits_i_tau: 32-bit integer, number of times expert i selected for task tau.
	•	loss_sum_i_tau: fixed-point sum of losses for expert i on task tau.
	•	loss_sq_sum_i_tau: fixed-point sum of squared losses.
	•	z_i_tau: fixed-point log-weight (prior preference or GRPO-corrected weight).

Global per-task counters:
	•	N_tau: 64-bit integer, total selections (sum_i visits_i_tau).
	•	step_global: 64-bit step counter.

All are stored in SHARED, within bandit_i and WORK.

7.2 Loss and Reward

Per step t, for task tau:
	•	Reward r_t in [R_min, R_max] (fixed, known range).
	•	Define normalized loss:
loss_t = (R_max - r_t) / (R_max - R_min)

so that loss_t in [0,1].

For selected expert i_t on task tau:
	•	loss_t(i_t) = loss_t
	•	loss_t(j) = 0 for j != i_t (unobserved arms).

7.3 Bandit Index (UCB-V style)

For each candidate expert i in C_large for task tau:
	1.	Let n_i = max(1, visits_i_tau).
	2.	Let L_i = loss_sum_i_tau.
	3.	Let Q_i = loss_sq_sum_i_tau.
	4.	Empirical mean loss:
hat_l_i = L_i / n_i
	5.	Empirical variance proxy:
var_i = max(0, Q_i / n_i - hat_l_i * hat_l_i)
	6.	Global time:
n_tot = max(1, N_tau)
	7.	Exploration bonus (UCB-V form):
bonus_i = alpha * sqrt( (2 * var_i * log(1 + n_tot)) / n_i )
+ alpha * (3 * log(1 + n_tot)) / n_i

where alpha > 0 is a fixed configuration constant (for example alpha = 1 in fixed-point scale).
	8.	Prior term via z_i_tau:
prior_i = -beta * z_i_tau

where beta >= 0 is a configuration constant.
	9.	Index to minimize:
index_i = hat_l_i - prior_i + bonus_i

Lower index_i means better expert.

7.4 BANDIT_CORE.CHOOSE

Input:
	•	task tau.
	•	candidate list C_large (expert slot indices).

Output:
	•	Either a single expert i_t or a subset C_t of size k_small.

Algorithm:
	1.	For each i in C_large, load bandit state (n_i, L_i, Q_i, z_i_tau).
	2.	Compute index_i as in 7.3 using fixed-point arithmetic.
	3.	If deterministic greedy:
	•	i_t = argmin_i index_i.
	4.	If stochastic:
	•	Center and scale indices into scores s_i and apply softmax with temperature T_bandit:
	•	p_i proportional to exp(-index_i / T_bandit).
	•	Sample i_t from p_i using RAND.
	5.	Optionally, to produce subset C_t:
	•	Sort or partially select k_small best experts by index_i.
	•	Return C_t.

Constraint:
	•	CHOOSE must run in bounded time independent of episode length, with complexity O(|C_large|) using integer arithmetic.

7.5 BANDIT_UPDATE_STEP

Once reward r_t is available for step t, for task tau and chosen expert i_t:
	1.	Compute loss_t in [0,1] as in 7.2.
	2.	Load (visits_i_tau, loss_sum_i_tau, loss_sq_sum_i_tau).
	3.	Update:
visits_i_tau <- visits_i_tau + 1
N_tau        <- N_tau + 1
loss_sum_i_tau <- loss_sum_i_tau + loss_t
loss_sq_sum_i_tau <- loss_sq_sum_i_tau + loss_t * loss_t
	4.	Optional incremental update to z_i_tau (simple prior toward low loss):
avg_l_i = loss_sum_i_tau / max(1, visits_i_tau)
delta_z = -eta_z * (avg_l_i - l_ref)

where:
	•	eta_z is a small learning rate in fixed-point.
	•	l_ref is a reference loss (for example 0.5).

	5.	Clip z_i_tau to [z_min, z_max].

All operations are integer or fixed-point; no floating point is required.

7.6 Theoretical Notes (Informative)
	•	With the UCB-V index in 7.3, per-task expected regret is O(sqrt(K T log T)) under standard stochastic bandit assumptions, where K is number of experts and T is number of selections for that task.
	•	The addition of bounded prior_i term via z_i_tau preserves no-regret up to constant factors as long as z_i_tau drifts are bounded and step-size eta_z is small.
	•	Implementation MAY plug in an exact Tsallis-INF or other best-of-both-worlds index in place of the UCB-V index, provided:
	•	INDEX(i,tau) depends only on (n_i, L_i, Q_i, z_i_tau, N_tau) and a fixed set of constants.
	•	INDEX is computable in integer arithmetic in O(1) per expert.
	•	Theoretical guarantees ensure sublinear regret.

	8.	Structural Updates and Templates

8.1 Logging

TRACE stores:
	•	Per-step entries:
	•	(time, task_id, context_hash, i_t, r_t, BAD_t, minimal obs_t).
	•	Per-episode summaries:
	•	returns, length, regret estimates.
	•	Candidate patches and metadata.
	•	Accepted patches with VC ids and PoX scores.

Each entry participates in a hash chain.

8.2 Error Localization

Given episodes or batches with low return or high regret, choose candidate step indices:

G(tau) subset of {0..T-1}

Examples:
	•	Last L steps of low-return episodes.
	•	Steps where regret surrogate exceeds threshold.

These steps guide where splits, merges, or macros might help.

8.3 Expert Split

Given i* used at t* in G(tau):
	1.	Choose gating bit index b from expert inputs or context bits.
	2.	Allocate new ids u0, u1 and slots i0, i1.
	3.	Define new experts i0, i1 with same R_spec and W_spec as i* and initial C_rep copies.
	4.	Gate:
	•	i0 active when bit b = 0.
	•	i1 active when bit b = 1.
	5.	Adjust routing:
	•	Deprecate i* or lower z_i*.
	•	Initialize z_i0, z_i1 near z_i*.
	•	Insert i0, i1 into ANN index.
	6.	Create patch Delta_split describing changes.

8.4 Expert Merge

For candidate merge cluster C_merge = {i_1,..,i_k}:
	1.	Evaluate similarity based on inputs, outputs, and stats.
	2.	If similar, define merged expert m:
	•	R_spec_m as intersection or union of R_spec_i_j.
	•	W_spec_m as compatible subset of W_spec_i_j.
	•	C_rep_m approximating aggregated behavior via search or CEGIS.
	3.	Update routing:
	•	Create new id u_m and slot i_m.
	•	Route most traffic to i_m; optionally keep residual experts.
	4.	Create patch Delta_merge.

8.5 Macro and Template Extraction

From logs L, extraction procedure T_extract(L) yields templates T_k:
	•	Represent CFG fragments or expert sequences that occur frequently.
	•	Characterized by:
	•	Sequence of nodes or experts.
	•	Parameter slots.
	•	Optional reward pattern.

Templates can be:
	•	Compiled into macro nodes.
	•	Used as teacher primitives for composite tasks.
	•	Targeted by superoptimization.

8.6 Superoptimization

For selected hot experts or macros:
	1.	Derive intermediate representation IR from C_rep and logs.
	2.	Define equivalence domain D.
	3.	Search for cheaper circuit C_rep’ equivalent to C_rep on D.
	4.	Construct VC_equiv(Delta) asserting equivalence.
	5.	Call SAT_CHECK on VC_equiv(Delta).
	6.	If UNSAT, accept patch Delta_superopt replacing C_rep with C_rep’.
	7.	Verification Kernel

9.1 CNF Representation

Variables var_j indexed by integers or subset of U. Clause c:
	•	Represented as pair (pos, neg), where pos_j = 1 if var_j appears positively, neg_j = 1 if var_j appears negatively.

CNF formula Phi is a finite set of clauses.

9.2 Hoare Logic Annotations

For each CFG node v:
	•	P_v is CNF formula for invariant at entry of v.

For edge (u, v) with instructions instr(u->v):
	•	WP_instr(u->v): mapping from CNF to CNF (weakest precondition).

Hoare consistency:

P_u = WP_instr(u->v)(P_v) for all edges (u, v) in the updated CFG.

9.3 VC Construction and Incremental Solving

For patch Delta:
	•	VC(Delta) is CNF capturing:
	•	Safety: BAD not reachable.
	•	Equivalence on domain D where required.
	•	Hoare consistency for updated CFG fragments.

Solver:
	•	baseCNF_id: baseline CNF.
	•	deltaCNF_id: clauses for Delta.

SAT_CHECK(base + delta) returns SAT, UNSAT, or UNKNOWN, with optional unsat_core_id.

9.4 CEGIS Integration

CEGIS(phi_id, hyp_class_id):
	•	Yields candidate satisfying phi or counterexample.

Counterexample can be mapped back to initial state and environment traces:
	•	Used to generate teacher tasks that expose the failure again.
	•	Used to refine experts or invariants.

CEGIS synthesizes:
	•	Experts from specifications.
	•	Invariants.
	•	Merged experts consistent with observed behavior.

	10.	PoX Objectives and Scheduling

10.1 PoX Score

For verified patch Delta, estimate:
	•	Delta_R: regret reduction across benchmark and teacher tasks.
	•	Delta_S: safety improvement (e.g. lower BAD rate).
	•	Delta_C: cost reduction (fewer cycles per step).
	•	Delta_I: information gain (diversity, teacher coverage).

PoX score:

score(Delta) = w_R * Delta_R + w_S * Delta_S + w_C * Delta_C + w_I * Delta_I

Weights w_* are stored in WORK.

10.2 PoX State and Difficulty

WORK segment stores:
	•	w_R, w_S, w_C, w_I.
	•	Difficulty D (acceptance threshold).
	•	Moving averages for score and yield.
	•	Saturation metrics (for example last_high_score_step).

10.3 Scheduler Policy

Scheduler chooses allocation fractions:

alpha_act, alpha_synth, alpha_verify in [0,1], sum to 1.

Heuristics:
	•	Increase alpha_synth when uncertainty or teacher novelty is high.
	•	Increase alpha_verify when many high-priority patches wait.
	•	Reduce alpha_verify when backlog low and yield low.
	•	Respect external requirements on alpha_act (must keep main agent running).

Concrete scheduler algorithm is given in Algorithm D.
	11.	Execution Model and Time Scales

11.1 Fast Path (Level 1)

Per-step loop:
	1.	STEP_FAST (Algorithm A).
	2.	BANDIT_UPDATE_STEP (Algorithm B) once reward is known.

No structural changes, teacher grammar changes, or heavy verification.

11.2 Mid Path

Executed periodically:
	•	GRPO_LITE_UPDATE (Algorithm C).
	•	Log aggregation and template extraction.
	•	Teacher curriculum updates.
	•	Drift metrics.

11.3 Slow Path (Level 2 and 3)

Executed asynchronously:
	•	Structural patch generation.
	•	PROVER calls and patch verification.
	•	PoX scoring.
	•	Scheduler meta-steps (Algorithm D).
	•	Snapshot and rollback.
	•	Teacher grammar extension and template installation.

	12.	Integrity and Audit

12.1 Log Chain

TRACE entries e_k form hash chain:

H_0 = fixed constant
H_{k+1} = Hash(H_k || e_k)

H_k is stored or exported, allowing tamper detection.

12.2 Merkle Trees

Merkle trees may cover:
	•	EXPERT segment.
	•	CFG.
	•	WORK configuration.
	•	TEACH configuration.

Update procedures recompute hashes from modified leaves upward. Root hashes are stored or exported.

12.3 Patch Acceptance

Patch Delta is applied only if:
	1.	VC(Delta) is verified: SAT_CHECK returns UNSAT.
	2.	PoX condition holds: score(Delta) >= D (if PoX enabled).

Accepted patches are logged with descriptors, VC identifiers, PoX scores, and parent hash.
	13.	Game-Theoretic Interpretation (Informative)

13.1 Stage Game

Players:
	•	Player 1: BEM controller (fast path).
	•	Player 2: environment plus teacher-generated tasks.

At time t:
	1.	Internal state X_t and observation obs_t.
	2.	Teacher selects a task instance (possibly implicit in environment).
	3.	BEM chooses expert i_t from available experts via BANDIT_CORE.CHOOSE.
	4.	State transitions via expert semantics to X_{t+1}.
	5.	Environment returns reward r_t and affects future observations.

Instantaneous payoff:

u_t = r_t - lambda_bad * BAD_t - lambda_cost * cost_t

13.2 Regret and Policies

For fixed task tau and action set A_tau:
	•	Policies map (obs, tau) to distributions over A_tau.
	•	Regret_T measures difference between cumulative payoff of BEM policy and best reference policy in a reference class, subject to safety constraints.

Design intent:
	•	Expected regret is sublinear in T under standard assumptions for each task.

13.3 Safety

Safety enforced by:
	•	BAD_t indicator.
	•	Verification constraints VC(Delta).
	•	Scheduler overrides (e.g. halting patch application when BAD rate increases).

13.4 Meta-Actions

Structural patches Delta and teacher grammar patches act as meta-actions transforming configuration C to C’ subject to:
	•	VC(Delta) being UNSAT.
	•	PoX threshold.

Meta-game payoff approximated by score(Delta) minus cost of verification and teacher compute.

13.5 Two-Time-Scale Learning

Fast time scale:
	•	Contextual bandit with safety constraints (fast path).

Slow time scale:
	•	Patch selection and verification via PoX.
	•	Teacher grammar evolution and curriculum bandits.
	•	Scheduler controlling compute allocation.

	14.	Self-Evolving Training Pipeline v0.0.1

14.1 Components
	•	B_main: main BEM with full verification and PoX.
	•	B_d: specialist BEMs per domain (math, code, reasoning, agentic, safety, etc).
	•	S_domain: domain scheduler controlling domain-level allocations.
	•	G_d,k: task generators per domain and difficulty in TEACH.
	•	V_d, E_d: domain-specific verifiers and evaluators.
	•	S_patch: patch scheduler managing queue Q_patch.

14.2 Phases

Phase 0: Initialization
	•	Initialize specialists with minimal expert sets and simple grammar primitives.
	•	Initialize B_main with slightly richer expert capacity.

Phase 1: Specialist self-play
	•	Run act and synth modes in each domain using G_d,k.
	•	Use bandit learning rules and GRPO-LITE to improve B_d.
	•	Log episodes and metrics.

Phase 2: Distillation into main BEM
	•	Construct distillation dataset from high-quality episodes from specialists.
	•	Train B_main (experts and bandit priors) to match expert choices and outputs.
	•	Install macros and templates via verified patches.

Phase 3: Mixed-domain RL on B_main
	•	Use S_domain and TEACH to choose domain, grammar, and mode (act, synth, verify).
	•	Run episodes, structural proposals, and verification.
	•	Use off-policy masking for unstable episodes.
	•	Use PoX and Algorithm D to select verification jobs.

Phase 4: Long-run self-evolution
	•	Maintain specialists for new domains or capabilities.
	•	Periodically distill into B_main.
	•	Continuously run mixed-domain RL, structural updates, and teacher curriculum updates.

	15.	Image Snapshot Model v0.0.1

15.1 Base Image

Define a stable base container image:
	•	Name: registry.example.com/bem-core:0.0.1
	•	Contents:
	•	BEM engine and CLI.
	•	SAT, ANN, and other libraries.
	•	Default configuration templates.

15.2 Long-lived Runtime Container

Run long-lived container:
	•	Name: bem-runner
	•	Derived from bem-core:0.0.1
	•	Command launches BEM scheduler and subsystems.

Inside container:
	•	/var/lib/bem: evolving engine state.
	•	/var/log/bem: logs.
	•	/etc/bem: configurations.

15.3 Snapshot as Image

Snapshot concept:
	•	Docker image created from filesystem of running bem-runner container.

Tag format examples:
	•	registry.example.com/bem-brain:YYYYMMDD-HHMM

Snapshot includes:
	•	All layers from base image.
	•	Top layer containing runtime state and configs.

15.4 Snapshot Scheduling and Gating

Scheduling:
	•	Snapshot agent triggers at fixed cadence or event-based.

Gating conditions:
	•	Read summary metrics from /var/lib/bem/metrics.
	•	Require:
	•	Zero safety violations in recent window.
	•	Verification backlog under threshold.
	•	Acceptable performance and PoX trends.

If gating fails, skip snapshot for this interval.

15.5 Restore and Promotion

Restore on another machine:
	•	docker pull bem-brain:TAG
	•	docker run –name bem-runner bem-brain:TAG bem-engine –resume

Promote snapshot to stable tag by re-tagging and pushing.

15.6 Migration to New Core

When base image version changes:
	•	Launch old snapshot container, archive /var/lib/bem.
	•	Launch new core container, import archive and run migration tool.
	•	Commit migrated container as new snapshot.

15.7 Retention and Security

Retention policy:
	•	Keep last N daily snapshots and M weekly snapshots and all stable tags.
	•	Clean up older non-stable snapshots.

Security:
	•	Use private registry.
	•	Restrict push and pull.
	•	Segregate external inputs via volumes where possible.

	16.	Core ISA v0.0.1

16.1 Registers

Scalar registers:
	•	x0..x31: 64-bit general purpose (x0 is zero).
	•	pc: program counter.
	•	sr: status register.
	•	seed: PRNG state.

Bitvector registers:
	•	b0..b31: W-bit vector registers.

Optional predicate registers:
	•	p0..p15: predicate flags.

Segment base registers (conceptual):
	•	sb_state, sb_shared, sb_expert, sb_cfg, sb_trace, sb_proof, sb_work, sb_teach.

16.2 Instruction Classes

Integer ALU:
	•	ADD rd, rs1, rs2
	•	SUB rd, rs1, rs2
	•	MULLO rd, rs1, rs2
	•	AND rd, rs1, rs2
	•	OR rd, rs1, rs2
	•	XOR rd, rs1, rs2
	•	SHL rd, rs1, imm
	•	SHR rd, rs1, imm
	•	SAR rd, rs1, imm
	•	NOT rd, rs1

Loads and stores:
	•	LD rd, [base + offset]
	•	ST rs, [base + offset]

Bitvector operations:
	•	BLD bd, [base + offset]
	•	BST bs, [base + offset]
	•	B_AND bd, bs1, bs2
	•	B_OR bd, bs1, bs2
	•	B_XOR bd, bs1, bs2
	•	B_NOT bd, bs1
	•	B_SHL bd, bs, imm
	•	B_SHR bd, bs, imm
	•	B_ROL bd, bs, imm
	•	B_ROR bd, bs, imm
	•	B_BLEND bd, mask, a, b
	•	B_POPCNT rd, bs
	•	B_PARITY rd, bs

Control flow:
	•	BEQ rs1, rs2, label
	•	BNE rs1, rs2, label
	•	BLT rs1, rs2, label
	•	BGE rs1, rs2, label
	•	JMP label
	•	JAL rd, label
	•	RET rd

Co-processor calls:
	•	COP op_id, rs_arg, rd_res

where op_id selects ANN, PROVER, HASH, LOG, TEACH, etc.

PRNG:
	•	RAND rd

updates seed and returns pseudo-random bits.

16.3 Co-Processor ABIs

ANN_QUERY:
	•	Input: pointer to descriptor with q, f pointer, k, config.
	•	Output: pointer to candidate list with count and slot indices.

PROVER_CALL:
	•	SAT_CHECK: cnf_id and options, returns result code and optional unsat_core_id.
	•	HOARE_CHECK, PROOF_CHECK, CEGIS similarly.

HASH_CALL:
	•	Input pointer and length, returns hash pointer.

LOG_CALL:
	•	Append log entry and update hash chain.

TEACH_CALL:
	•	Input: teacher request (domain, difficulty, curriculum context).
	•	Output: task descriptor id, to be interpreted by CFG and environment.

	17.	Algorithms (Normative Pseudocode)

17.1 Algorithm A: STEP_FAST

Input:
	•	Current state (s_t, M_t).
	•	task_id_t and context_hash_t (from previous step or teacher).

Pseudocode:
	1.	obs_t = O(s_t, M_t)
	2.	(q_t, f_t) = (F_id(task_id_t, context_hash_t), feature_bits_t)
	3.	Build ANN descriptor D_ann with (q_t, f_t, k_large, config(task_id_t))
	4.	C_large = ANN_QUERY(D_ann)
	5.	i_t = BANDIT_CORE.CHOOSE(task_id_t, C_large)
	6.	Call BEM_EXPERT_BATCH(i_t, STATE, SHARED) to obtain (s_{t+1}, M_{t+1})
	7.	Log minimal info to TRACE:
	•	(task_id_t, context_hash_t, i_t, fast_path_cost_proxy)
	8.	Prepare bandit update context:
	•	Store i_t and task_id_t in SHARED as last_choice
	9.	Compute new context_hash_{t+1} from (context_hash_t, obs_t, last_reward_t, i_t)
	10.	Return

Note:
	•	Reward r_t may be available only at episode end or after a delay. BANDIT_UPDATE_STEP uses last_choice and r_t when known.

17.2 Algorithm B: BANDIT_UPDATE_STEP

Input:
	•	task tau.
	•	expert i_t (stored as last_choice).
	•	reward r_t in [R_min, R_max].

Pseudocode:
	1.	Load bandit state for (i_t, tau):
	•	n_i = visits_i_tau
	•	L_i = loss_sum_i_tau
	•	Q_i = loss_sq_sum_i_tau
	2.	Compute loss_t = (R_max - r_t) / (R_max - R_min)
	3.	visits_i_tau = n_i + 1
	4.	N_tau = N_tau + 1
	5.	loss_sum_i_tau = L_i + loss_t
	6.	loss_sq_sum_i_tau = Q_i + loss_t * loss_t
	7.	Optional:
	•	avg_l_i = loss_sum_i_tau / max(1, visits_i_tau)
	•	delta_z = -eta_z * (avg_l_i - l_ref)
	•	z_i_tau = clip(z_i_tau + delta_z, z_min, z_max)
	8.	Store updated values back to SHARED

17.3 Algorithm C: GRPO_LITE_UPDATE

Executed mid-path every T_group episodes or steps.
	1.	Partition episodes into groups by (task_id, initial_context_bucket).
	2.	For each group g:
	•	Compute returns R_j for episodes j in g.
	•	Compute mean mu_g and std sigma_g of R_j.
	•	For each episode j:
	•	A_j = (R_j - mu_g) / max(sigma_g, epsilon)
	3.	For each expert i, task tau:
	•	Accumulate A_bar_i_tau as mean advantage over episodes where i was used.
	4.	For each (i, tau):
	•	z_i_tau = clip(z_i_tau + eta_grpo * A_bar_i_tau, z_min, z_max)

All operations are in fixed-point.

17.4 Algorithm D: Contextual Scheduler META_STEP

Arms:
	•	act: run STEP_FAST episodes with current tasks.
	•	synth: run teacher and structural proposal generation.
	•	verify: run PROVER and patch application.

State:
	•	For each arm a:
	•	A_a: d_ctx x d_ctx matrix (approximate covariance).
	•	b_a: d_ctx vector.
	•	For each arm a, maintain reward estimates.

Pseudocode:
	1.	Build context x_m from WORK and TEACH:
	•	includes backlogs, PoX yield, safety_violation_rate, teacher novelty, etc.
	2.	For each arm a:
	•	Solve A_a * theta_a = b_a approximately by a fixed number of iterations of a linear solver.
	•	Compute predicted reward r_hat_a = dot(theta_a, x_m)
	•	Compute exploration bonus ucb_a = c_sched * sqrt(dot(x_m, A_a^{-1} x_m)) approximated.
	•	score_a = r_hat_a + ucb_a
	3.	Apply safety overrides:
	•	If safety_violation_rate > threshold, suppress synth and verify if needed.
	4.	Choose arm a_star with maximum score_a.
	5.	Execute one workload unit of a_star:
	•	act: run several episodes of STEP_FAST + BANDIT_UPDATE_STEP.
	•	synth: run teacher and patch generation on TRACE.
	•	verify: run PROVER on pending patches.
	6.	Observe meta-reward r_meta (e.g. improvement in PoX yield or regret).
	7.	Update A_a_star and b_a_star:
	•	A_a_star = A_a_star + x_m x_m^T
	•	b_a_star = b_a_star + r_meta * x_m

17.5 Algorithm TEACH_STEP (Task Generator Core)

TEACH maintains a grammar over task templates. Each template t has:
	•	difficulty estimate d_t.
	•	pass_rate_t (moving average).
	•	usage_t (count).

Task templates are built from primitives:
	•	base problems (simple parity, majority, key-value, bandit, etc).
	•	composition operators:
	•	SEQ(a,b): sequential composition.
	•	PAR(a,b): parallel subtasks in different bits or lanes.
	•	NEST(a,k): nesting of a to depth k.
	•	MASK(a,mask_pattern): masked or corrupted variant.
	•	INTERLEAVE(a,b): mixed streams.

TEACH_STEP:
	1.	For each template t, maintain bandit state (similar to experts) over meta-reward:
	•	meta_reward_t = function of (pass_rate_t, regret_t, novelty_t)
	2.	For requested domain d and difficulty band k:
	•	Collect candidate templates T_d,k with d_t near target difficulty.
	•	Use TEACH_BANDIT.CHOOSE (same core as BANDIT_CORE) to select template t*.
	3.	With probability p_mut:
	•	Sample small mutations of t* via grammar operators up to depth D_task.
	•	Form new template t_new.
	•	Initialize its stats.
	4.	Return t_selected (t* or t_new) as teacher_task_desc_t.
	5.	Update pass_rate_t and meta bandit stats after task completion:
	•	pass_rate_t via exponential moving average.
	•	meta bandit stats via TEACH_BANDIT.UPDATE.

Constraints:
	•	Teacher bandit uses the same no-regret core as BANDIT_CORE but with different reward definition.
	•	Generation depth and branching factor are bounded so TEACH_STEP cost is bounded.

End of BEM v0.0.1 integrated spec.
