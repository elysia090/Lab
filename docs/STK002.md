Title
Sera Transfer Kit v0.0.2 – Deterministic CPU Projection to Sera v0.0.1+ Runtime

Status
Draft, implementation-grade, self-contained

Language
ASCII, English only
	0.	Conventions, global goals, and non-goals

0.1 Mathematical and type conventions

R           : real numbers
N           : {0,1,2,…}
N+          : {1,2,…}
R^{m x n}   : real matrix m by n
||·||_2     : Euclidean norm
||·||_F     : Frobenius norm
sigma(x)    : 1 / (1 + exp(−x))
u64         : 64-bit unsigned integer

Indexing is zero-based.

0.2 Global goals

The Sera Transfer Kit (STK) converts a frozen Transformer checkpoint into a Sera v0.0.1+ model artifact with the following guarantees:
	1.	Determinism
For fixed inputs and configuration:
	•	All arrays and manifest fields are bitwise reproducible.
	•	All internal tests produce identical results.
	2.	CPU-only and bounded resources
	•	No GPU usage.
	•	Each stage is O(P_total) over the number of scalar parameters.
	•	Peak RAM is bounded by max_layer_bytes + scratch_budget with a fixed scratch_budget.
	3.	Runtime compatibility
	•	Output arrays and manifest fields match the Sera v0.0.1+ ABI:
	•	O(1) tokenizer.
	•	PRF attention module.
	•	Injective linear module.
	•	Finite rational memory.
	•	Optional attention overlays and FFN low-rank blocks.
	4.	Constant-time runtime hot path
	•	No runtime data structures require rehashing or resizing.
	•	All per-event Sera operations remain O(1).

0.3 Non-goals

STK does not:
	•	Perform any training or fine-tuning.
	•	Change model topology.
	•	Infer or configure Sera’s bridge, trust gate, or search parameters.
	•	Guarantee preservation of task performance; it preserves structure and approximates relevant kernels under explicit tolerances.

	1.	High-level mapping

1.1 Architecture assumptions

The input model is a single or multi-layer Transformer with:
	•	Token embedding E_tok in R^{vocab x d_model}.
	•	Optional tied output projection W_out in R^{d_model x vocab}.
	•	L layers with:
	•	Multi-head self-attention (MHSA) Q,K,V,O weights:
W_Q, W_K, W_V in R^{n_heads x d_head x d_model},
W_O in R^{d_model x n_heads x d_head} or [d_model x d_model].
	•	Feed-forward network per layer:
W1 in R^{d_ffn x d_model}, b1 in R^{d_ffn},
W2 in R^{d_model x d_ffn}, b2 in R^{d_model}.
	•	Positional encoding of one of:
	•	RoPE,
	•	ALiBi,
	•	learned absolute embeddings,
	•	or none (positional handled externally).

1.2 Mapping to Sera v0.0.1+ modules

STK produces:
	•	Tokenizer module:
	•	O(1) tokenizer tables compatible with Sera’s tokenizer state.
	•	PRF attention module:
	•	prf_W, whitening_mu, whitening_sig2, tau, beta_floor.
	•	Linear module:
	•	A lossless hashed linear dictionary for:
	•	Output heads (logits/regression).
	•	Optional FFN low-rank blocks.
	•	Finite rational memory module:
	•	ARMA/SOS coefficients approximating positional kernel.
	•	Optional attention overlays:
	•	Fixed low-rank value corrections for PRF attention derived from W_O.

STK does not emit or configure:
	•	Bridge arrays (hubs, qDin/qDout).
	•	Trust-gate keys or parameters.
	•	Search (MCTS/CFR) parameters.

Those are separately configured in Sera and can treat STK output as a pure base model.
	2.	Inputs and configuration

2.1 Required files

Input directory layout:
	•	input/model.safetensors
or
	•	input/pytorch_model.bin
	•	input/config.json containing at least:
	•	“d_model”: int
	•	“n_layers”: int
	•	“n_heads”: int
	•	“d_ffn”: int or list per layer
	•	“vocab_size”: int
	•	“positional_encoding”: “rope” | “alibi” | “learned_abs” | “none”
	•	optional fields:
	•	“rope_theta”: float or list
	•	“alibi_slopes”: array
	•	“layernorm_eps”, “activation”: “gelu” | “relu” | …

2.2 Optional tokenizer files

At least one of:
	•	input/tokenizer.model (e.g. sentencepiece).
	•	input/merges.txt and input/vocab.json (BPE).

If none exist:
	•	STK constructs a trivial byte-level tokenizer:
	•	Vocabulary = all bytes 0..255;
	•	L_tok = 1;
	•	No FST; direct mapping.

2.3 Configuration knobs (CLI or config)

STK takes configuration constants:
	•	r_prf              (default 512)
	•	tol_prf            (default 1e-2)
	•	head_mode          = “head-only” | “head+ffn-lowrank”
	•	r_ffn              (default 32)
	•	tol_ffn            (default 5e-2)
	•	T_fit_pos          (default 128)
	•	p_max, q_max, L_max_SOS (default small integers, e.g. p<=4, q<=2, L<=6)
	•	root_seed          (u64, default fixed constant if not supplied)
	•	scratch_budget     (MiB, default 512)
	•	threads            (int, default number of physical cores).

All these values are recorded into the manifest so that replay is possible.
	3.	Output artifact structure

3.1 Directory layout

Output directory:
	•	output/sera_manifest.bin
	•	output/arrays/tokenizer_fst.bin
	•	output/arrays/tokenizer_T_n.bin (for n=1..L_tok)
	•	output/arrays/prf_W.bin
	•	output/arrays/whitening_mu.bin
	•	output/arrays/whitening_sig2.bin
	•	output/arrays/linear_mphf.bin
	•	output/arrays/linear_keys.bin
	•	output/arrays/linear_weights.bin
	•	output/arrays/cuckoo_delta.bin
	•	output/arrays/memory_coeff.bin
	•	output/arrays/delaybuf_init.bin

Optional arrays:
	•	output/arrays/overlays_H.bin
	•	output/arrays/overlays_U.bin
	•	output/arrays/overlays_DeltaW.bin
	•	output/arrays/ffn_lowrank.bin
	•	output/arrays/ffn_meta.bin

3.2 Common array header

Each .bin in output/arrays uses the 64-byte header defined earlier, followed by raw data.

The header’s crc32c and sha256_low fields cover only the payload bytes, not the header.

3.3 Manifest layout

Manifest is a single fixed binary record with:
	•	magic, endian, abi_flags, schema_hash.
	•	run_metadata (threads, root_seed, library identifiers).
	•	tokenizer section.
	•	prf section.
	•	denominator section.
	•	linear section.
	•	memory section.
	•	overlays section.
	•	ffn section.
	•	reserved sections for:
	•	bridge, trust, search, capacity (zeroed).

Type for each field is fixed-size integer or IEEE 754 float in little-endian form.

STK computes schema_hash as SHA-256 low 64 bits of a static schema description string hard-coded into the implementation. The Sera loader checks schema_hash to ensure compatibility.
	4.	Determinism and numeric contract

4.1 PRNG

PRNG core: splitmix64 with state s in u64.

splitmix64(s):
s = s + 0x9E3779B97F4A7C15
z = s
z = (z ^ (z >> 30)) * 0xBF58476D1CE4E5B9
z = (z ^ (z >> 27)) * 0x94D049BB133111EB
z = z ^ (z >> 31)
return (s, z)

Each stage uses a derived initial state:

s_stage = hash64(root_seed, stage_id, layer_id, local_idx)

where hash64 is a fixed mixing function (e.g. another splitmix64 pass over a concatenation of components).

There is no data-dependent reseeding.

4.2 Gaussian sampling and uniforms

Convert 64-bit integer z to uniform u in (0,1):

u = ((z >> 11) * 2^(−53))
if u <= 0: u = 2^(−53)
if u >= 1: u = 1 − 2^(−53)

For Box-Muller:

Given u1, u2 in (0,1):

r = sqrt(−2 log(u1))
theta = 2 pi u2
g0 = r cos(theta)
g1 = r sin(theta)

cos and sin are taken from a fixed libm implementation; the abi_flags record which implementation and compiler options were used.

4.3 Threading and BLAS

STK sets:
	•	OMP_NUM_THREADS, MKL_NUM_THREADS (or equivalents) = threads.
	•	Pinning policy is fixed per implementation and recorded in manifest.abi_flags.

No multi-threading is used in a way that affects floating-point reduction order beyond what is captured in the recorded environment.

4.4 Floating-point modes

STK uses:
	•	Double precision (f64) for:
	•	PRF kernel tests,
	•	SVD/QR and rational fitting,
	•	accumulation in GEMMs (if using BLAS dgemm or sgemm+compensated sums).
	•	Single precision (f32) for:
	•	Stored weights in arrays unless a module requires f64 (e.g. memory_coeff).

The Sera loader must know from dtype fields how to interpret stored arrays.

FTZ (flush-to-zero) and denorm mode are fixed, recorded in abi_flags, and not changed during execution.
	5.	Tokenizer projection

5.1 Vocabulary extraction

Case 1: sentencepiece model.
	•	STK reads all pieces and decodes them to UTF-8 byte sequences.
	•	Discards special tokens that are not used for text (e.g. , , ) if configured; otherwise includes them as normal pieces.

Case 2: merges.txt + vocab.json.
	•	STK reconstructs wordpieces; each vocab entry yields a bytepiece string.
	•	For pieces longer than L_tok bytes, STK either:
	•	splits them into subpieces consistent with merges; or
	•	discards them if they are unreachable by typical BPE decoding.

Resulting vocabulary:

V = {v_0, v_1, …, v_{K−1}} where each v_i in {0..255}^len_i and 1 <= len_i <= L_tok.

5.2 Sardinas–Patterson uniqueness

STK builds SP sets:

S_0 = {v_i v_j^{-1} | v_i, v_j in V, v_i != v_j and v_j is prefix of v_i}
S_{k+1} = { x v_i^{-1} | x in S_k, v_i in V, v_i is prefix of x }
union { v_i x | x in S_k, v_i in V, x is prefix of v_i }

If epsilon (empty string) appears in any S_k, V is not uniquely decodable and STK aborts.

Otherwise:
	•	STK accumulates a canonical representation of all S_k strings (e.g. sorted list of strings plus lengths) and computes sp_cert_digest as SHA-256 of that representation.

5.3 Normalizer N and FST encoding

Normalizer N is built as:
	•	A deterministic finite-state transducer with:
	•	States representing Unicode normalization context.
	•	Transitions encoding:
	•	UTF-8 validation,
	•	control/ZWJ/zero-width spaces handling,
	•	canonical composition to NFC,
	•	mapping of confusables.

STK compiles N into:
	•	tokenizer_fst.bin, containing:
	•	Number of states S_norm.
	•	Transition table:
	•	For each state and input byte, next_state, output action.
	•	Lookahead L_norm (<=4) for NFC finalization.

Sera’s tokenizer uses this FST exactly; STK does not depend on Sera’s internal implementation.

5.4 Rolling hash and MPHF construction

For each n in 1..L_tok:
	•	Compute all tokens of length n: V_n = {v_i | len(v_i) = n}.
	•	For each v in V_n, compute a 64-bit base hash H_base(v) using a fixed function (e.g. splitmix64 over bytes).

Partition V_n into M buckets:
	•	M_n = ceil(alpha_mphf * |V_n|) with alpha_mphf ≈ 1.23.
	•	bucket_id(v) = H_base(v) mod M_n.

For each bucket b:
	•	Let B_b = {v in V_n | bucket_id(v) = b}.
	•	STK searches for a seed s_b in [0, 2^16−1] such that:
For each v in B_b:
pos(v) = (mix64(H_base(v), s_b) mod M_n, possibly with secondary attempts)
All pos(v) are distinct and not used by previous buckets.
	•	Search increments s_b deterministically.

If search fails for some bucket:
	•	STK increases M_n by a fixed factor (e.g. +5%) and repeats.

The resulting per-bucket seeds and the per-length base table are serialized to tokenizer_T_n.bin. Decoding equality (bytewise match of Dec[id]) is enforced at runtime by Sera.

5.5 Encoder constraints

For Sera’s runtime encoder:
	•	Each normalized byte triggers at most L_tok probes across T_n.
	•	There is no backtracking; accept-first match.
	•	Residual bytes at end are flushed as single-byte tokens.

These properties are ensured by construction (prefix/unique decode and bounded windows).
	6.	PRF attention projection

6.1 Statistics extraction from W_Q, W_K

For each attention layer:
	•	Gather W_Q, W_K across heads into matrices of shape [n_heads * d_head x d_model].
	•	Compute normalized Frobenius norms:
f_Q = ||W_Q||_F / sqrt(n_heads * d_head * d_model)
f_K = ||W_K||_F / sqrt(n_heads * d_head * d_model)

Approximate variance of q^T k:
	•	For h_in ~ N(0, I), q = W_Q h_in, k = W_K h_in:
Var(q^T k) ≈ f_Q^2 f_K^2 d_model

STK aggregates these statistics across layers (e.g. median or mean per layer).

6.2 Tau selection

Given an approximate scale s_att = sqrt(Var(q^T k)), STK chooses tau such that:
	•	P(|q^T k / tau| > T_max) is small for T_max ≈ 5..8.

One simple rule:
	•	tau = max(tau_min, s_att / T_target)

where:
	•	tau_min is a floor (e.g. 0.1).
	•	T_target is a constant (e.g. 4 or 5).

tau is written to manifest.prf.tau.

6.3 prf_W generation

STK builds prf_W in R^{r x d_model}:

Algorithm:

build_prf_W(d_model, r, root_seed):

s = hash64(root_seed, stage_id=PRF, layer_id=0, local_idx=0)
for i in 0..r−1:
for j in 0..d_model−1 step 2:
(s, z1) = splitmix64(s)
(s, z2) = splitmix64(s)
u1 = to_unit(z1)
u2 = to_unit(z2)
(g0,g1) = box_muller(u1,u2)
prf_W[i,j]   = g0
if j+1 < d_model:
prf_W[i,j+1] = g1

Optionally apply diagonal scaling derived from W_K:
	•	Use diag D with elements d_j >= eps_wk, where d_j = function of W_K’s column norms.
	•	Replace prf_W[i,j] with prf_W[i,j] * sqrt(d_j) to align scales.

Store prf_W as f32.

6.4 Whitening initialization

STK sets:

whitening_mu[j]   = 0 for j in 0..r−1
whitening_sig2[j] = 1 for j in 0..r−1

or, optionally:
	•	Compute v = prf_W 1 (vector of sums of each row).
	•	Normalize v.
	•	Set whitening_sig2[j] = 1 + alpha_wh |v_j|.

whitening_eps in manifest.prf ensures safe division at runtime.

6.5 Kernel error test

STK validates PRF kernel approximation:

For k in 1..K_test (e.g. 1024):
	•	Sample q,k ~ N(0, I_{d_model}).
	•	Compute in double:
dot = q^T k
true_k = exp(dot / tau)
	•	Compute phi(q), phi(k) in double:
For each i:
u_q = prf_W[i]^T q / sqrt(tau)
u_k = prf_W[i]^T k / sqrt(tau)
phi_q[i] = r^(−1/2) exp(u_q − ||q||^2/(2tau))
phi_k[i] = r^(−1/2) exp(u_k − ||k||^2/(2tau))
prf_k = sum_i phi_q[i] phi_k[i]

Collect vectors true_k[k] and prf_k[k]. Compute:

err_rel = ||prf_k − true_k||_2 / ||true_k||_2

If err_rel <= tol_prf, accept. Else:
	•	If r < r_prf_max:
	•	Increase r, rebuild prf_W, re-test.
	•	Else:
	•	Record degraded flag in manifest.overlays and manifest.prf.
	•	Sera may still run; approximate kernel is weaker.

	7.	Linear head projection (lossless hashed linear)

7.1 Key scheme

For each scalar weight w in:
	•	Output head W_out (if present).
	•	Optional other linear heads explicitly selected by configuration.

STK defines a semantic key:

key_sem = (HEAD_ID, out_idx, in_idx)

where:
	•	HEAD_ID identifies which head (e.g. logits head, regression head).
	•	out_idx is the output coordinate index.
	•	in_idx is the input feature index in [0..d_model−1].

STK encodes key_sem into a 64-bit integer:

key_u64 = hash64(“HEAD”, HEAD_ID, out_idx, in_idx)

The encoding is injective for the supported index ranges.

7.2 Thresholding and base key set

STK includes a weight w if:
	•	|w| >= tau_low_linear

tau_low_linear is chosen via:
	•	For each head, compute distribution of |W_out| entries.
	•	Set tau_low_linear to a quantile (e.g. p90) or absolute threshold ensuring:
Number of live keys C_base <= C_target

for some C_target set by configuration (or by Sera’s linear capacity C).

Live key set:

K_base = { key_u64 | |w| >= tau_low_linear }.

7.3 MPHF construction

STK builds a minimal perfect hash function:

h_base: K_base -> [0..C−1]

with:
	•	C >= |K_base| and C <= C_max determined by manifest.linear.C.

Construction uses a bucket scheme similar to tokenizer MPHFs, but with:
	•	Potential multi-stage hashing to avoid collisions.
	•	Hard deterministic caps on per-bucket search work.

Constraints:
	•	h_base is injective on K_base.
	•	No collisions are allowed; otherwise STK increases C and rebuilds.

7.4 Data layout

linear_weights.bin stores:
	•	weights[slot] = w for key mapped to slot.

linear_keys.bin (optional) stores:
	•	key_u64[slot] for audit and debugging; Sera runtime need not load it.

linear_mphf.bin stores:
	•	Global metadata: C, bucket count, seeds, function parameters.

Sera’s injective linear module uses:
	•	Provided MPHF function h_base and linear_weights to implement READ, ADD semantics on base dictionary indices. STK itself builds only the initial weights; ADD is runtime.

7.5 Delta dictionary (cuckoo)

Weights below tau_low_linear are ignored by STK projection. If Sera wants to reconstruct or refine them:
	•	Sera can use a bounded cuckoo delta structure with parameters from manifest.linear.{b,S,L_cuckoo,Q}.

STK does not populate cuckoo_delta for teacher weights; it is left empty (all zero) at transfer time so runtime can use it for incremental learning only.
	8.	Optional FFN low-rank projection

8.1 Scope

FFN projection is enabled if:
	•	head_mode = “head+ffn-lowrank”.

If disabled, ffn_lowrank.bin and ffn_meta.bin contain empty structures and manifest.ffn.enable_lowrank = 0.

8.2 Linearized Jacobian approximation

For each layer with W1, W2:
	•	Approximate the FFN as a linear map near the origin:
J_approx = alpha_act * W2 * W1

where alpha_act depends on activation type:
	•	For GELU:
	•	phi(x) ≈ 0.5 x near 0 ⇒ alpha_act ≈ 0.5.
	•	For ReLU:
	•	phi’(0)=0 but with symmetric noise, effective slope ≈ 0.5; configuration may override.
	•	For other activations, a fixed table of slopes is used.

8.3 Truncated SVD

STK computes truncated SVD of J_approx:
	•	J_approx ≈ U_ffn S_ffn V_ffn^T

with:
	•	U_ffn in R^{d_model x r_ffn}
	•	S_ffn in R^{r_ffn}
	•	V_ffn in R^{d_model x r_ffn}

Algorithm must:
	•	Use deterministic bidiagonalization and Householder reflectors.
	•	Fix sign convention (e.g. diagonal entries of S_ffn nonnegative, sign absorbed into U_ffn).

8.4 Error control

STK measures compression error on K_dir random directions u_k:

For k in 1..K_dir:
	•	Sample u_k ~ N(0, I_{d_model}); normalize u_k.
	•	y_true = J_approx u_k
	•	y_lr   = U_ffn diag(S_ffn) V_ffn^T u_k

Compute:

err_k = ||y_true − y_lr||_2 / max(||y_true||_2, eps_norm)

Let err_max = max_k err_k. If err_max <= tol_ffn:
	•	Accept low-rank block; else:
	•	Either reduce r_ffn and retry, or
	•	Disable low-rank compression for this layer.

Decision policy is deterministic and recorded in ffn_meta.bin for each layer.

8.5 Storage and Sera usage

ffn_lowrank.bin stores:
	•	For each accepted layer l:
	•	U_l in R^{d_model x r_ffn_l}
	•	S_l in R^{r_ffn_l}
	•	V_l in R^{d_model x r_ffn_l}

ffn_meta.bin stores:
	•	Per-layer offsets into ffn_lowrank.bin, ranks r_ffn_l, and flags.

Sera runtime can:
	•	Treat these blocks as an additional φ contribution or as part of external feature computation. STK does not constrain how they are used, only that the representation is deterministic and rank-limited.

	9.	Finite rational memory projection

9.1 Target impulse response

STK constructs a target impulse response h[t] for t in {0..T_fit_pos}:

Case rope:
	•	Use the complex rotation defined by RoPE base θ per head.
	•	For a fixed reference vector, compute how logit contributions decay with relative position t.
	•	Aggregate across heads (e.g. mean) to get h[t].

Case alibi:
	•	Use slope parameters m_head; define h[t] from m_head * t aggregated across heads.

Case learned_abs:
	•	Compute absolute embeddings p_pos; define:
h[t] = mean_over_heads( <p_pos, p_{pos+t}> − <p_pos, p_pos> )

for some range of pos indices where both pos and pos+t are valid.

Case none:
	•	h[t] = delta[t] (identity), trivial; memory module effectively disabled.

9.2 Model family

STK uses either:
	•	ARMA(p,q) model:
y_t + a_1 y_{t−1} + … + a_p y_{t−p}
= b_0 u_t + … + b_q u_{t−q}

or
	•	SOS with L sections (each AR(2) or AR(1)):
y_t = sum_{k=1..L} y_t^{(k)}, each y_t^{(k)} satisfying its own AR(2)/AR(1).

The mode and orders are bounded:
	•	1 <= p <= p_max, 0 <= q <= q_max.
	•	1 <= L <= L_max_SOS.

9.3 Fitting algorithm

Vector-fit-like process in discrete time:

fit_rational(h, T_fit_pos, mode, order limits):

Initialize poles z_i on a circle of radius rho_init < 1:
e.g. z_i = rho_init * exp(2 pi i / M) for some M.

For iteration it in 1..iter_max:
- Build linear system relating residues and numerator coefficients to h[t].
- Solve least-squares in double precision.
- Optionally update poles via rational approximation step.
- Enforce |z_i| < 1 by scaling (see 9.4).

Extract coefficients (a_j, b_k) or section parameters.

iter_max is small (e.g. 10); if reached, accept current coefficients.

9.4 Stability enforcement

For each pole z_i:
	•	If |z_i| >= 1, set:
z_i’ = z_i * scale with scale < 1 (e.g. 0.95)

Repeat at most K_stab iterations; if any pole remains with |z_i| >= 1:
	•	Reject that order and try a lower-order model or different mode (e.g. switch from ARMA to SOS).

9.5 Quality check

Construct approximate impulse g[t] by simulating the fitted model with unit impulse input and zeros initial state. Compute:

cos_sim = <h,g> / (||h||_2 ||g||_2)

If cos_sim >= cos_min:
	•	Accept; else:
	•	Try lower order or simpler model.
	•	If none meets cos_min, set manifest.memory.mode to “none” and use trivial identity.

9.6 Storage

memory_coeff.bin stores:
	•	mode: enum { 0=NONE, 1=ARMA, 2=SOS }.
	•	p,q or L.
	•	Coefficient arrays a_j, b_k or per-section parameters.
	•	All in double precision.

delaybuf_init.bin stores initial state; typically zeros for all states.

Sera runtime can load these arrays directly into its finite rational memory module.

manifest.memory records p,q,L, T_phi, K, pole_radius_min derived from poles.
	10.	Capacity and linear thresholds (consistency with Sera)

10.1 Capacity C and tau thresholds

STK must choose C and (tau_low, tau_high) such that Sera’s capacity condition is satisfiable at runtime:

C * (tau_high − tau_low) >= lambda_max * T_rb + margin

where:
	•	lambda_max is a target upper bound on new-key rate measured or specified.
	•	T_rb is the worst-case rebuild time for the linear store (specified by Sera).
	•	margin is a safety factor.

At transfer time, STK sees no runtime λ; it chooses:
	•	tau_low_linear (projection threshold).
	•	tau_low_scheduling >= tau_low_linear.
	•	tau_high_scheduling > tau_low_scheduling.

manifest.linear.tau_low, tau_high hold tau_low_scheduling, tau_high_scheduling.

Sera configures lambda_max and T_rb; if the inequality fails after runtime measurement, Sera can freeze inserts and require a re-projection with larger C.

10.2 Base vs delta

At STK time:
	•	linear_weights encodes only base dictionary; delta is empty.
	•	tau_low is the runtime threshold for “hot” vs “cold” features; it may be chosen lower than tau_low_linear to allow runtime learning to add new keys.

manifest.linear.C is set to:
	•	At least |K_base| / load_factor, with load_factor <= 0.9.

b, S, L_cuckoo, Q are set to standard values ensuring bounded probe lengths and relocations.
	11.	Schema, versioning, and ABI compatibility

11.1 Versioning

STK version is recorded in manifest as:
	•	major = 0
	•	minor = 0
	•	patch = 2

Socalled “v0.0.2”. ABI-breaking changes increment major or minor. Schema-compatible refinements increment patch only.

11.2 Schema hash

schema_hash is computed as SHA-256 low 64 bits of a fixed ASCII string describing:
	•	Array names and header format.
	•	Manifest field layout in byte order.
	•	Enum values.

Sera checks schema_hash and must reject artifacts whose schema_hash is unknown.

11.3 Backward compatibility

If Sera v0.0.1+ expects additional arrays (e.g. overlays_H) which STK does not emit:
	•	Sera treats missing arrays as “module disabled” when enable flag is 0.
	•	All fields used by Sera for enabled modules must be present; STK ensures this.

STK must not emit partially filled structures for enabled modules; failure implies module disabled with explicit flags.
	12.	Stage pseudocode

12.1 Top-level convert

function convert(in_dir, out_dir, cfg):
cfg = load_cfg(in_dir, cfg)
tensors = open_model(in_dir, cfg)
token_src = detect_tokenizer(in_dir)
manifest = init_manifest(cfg)
build_tokenizer(token_src, out_dir, manifest, cfg)
build_prf(tensors, out_dir, manifest, cfg)
project_heads(tensors, out_dir, manifest, cfg)
if cfg.head_mode == “head+ffn-lowrank”:
project_ffn_lowrank(tensors, out_dir, manifest, cfg)
fit_rational_memory(tensors, out_dir, manifest, cfg)
finalize_manifest(out_dir, manifest)
run_checks(out_dir, manifest, cfg)

12.2 build_tokenizer

function build_tokenizer(token_src, out_dir, manifest, cfg):
if token_src.kind == “sentencepiece”:
V = extract_pieces_sp(token_src)
else if token_src.kind == “bpe”:
V = extract_pieces_bpe(token_src)
else:
V = byte_level_vocab()
ensure_len_bounds(V, L_tok_max=cfg.L_tok)
sp_cert_digest = sardinas_patterson_cert(V)
fst = build_normalizer_fst()
tables = build_mphf_tables(V, cfg)
write_tokenizer_arrays(out_dir, fst, tables)
update_manifest_tokenizer(manifest, V, fst, tables, sp_cert_digest)

12.3 build_prf

function build_prf(tensors, out_dir, manifest, cfg):
stats = compute_attn_stats(tensors)
tau = choose_tau(stats, cfg)
prf_W = build_prf_W(cfg.d_model, cfg.r_prf, cfg.root_seed, stats)
(err_rel, ok) = test_prf_kernel(prf_W, tau, cfg)
if not ok and cfg.r_prf < cfg.r_prf_max:
cfg.r_prf = cfg.r_prf_max
prf_W = build_prf_W(…)
(err_rel, ok) = test_prf_kernel(…)
whitening_mu, whitening_sig2 = init_whitening(prf_W, cfg)
write_prf_arrays(out_dir, prf_W, whitening_mu, whitening_sig2)
update_manifest_prf(manifest, tau, cfg.r_prf, err_rel, ok)

12.4 project_heads

function project_heads(tensors, out_dir, manifest, cfg):
W_heads = gather_heads(tensors, cfg)
(keys, weights) = select_live_head_weights(W_heads, cfg)
mphf = build_mphf(keys, cfg)
linear_weights = assign_slots(mphf, keys, weights, cfg)
write_linear_arrays(out_dir, mphf, keys, linear_weights)
update_manifest_linear(manifest, mphf, cfg)

12.5 project_ffn_lowrank

function project_ffn_lowrank(tensors, out_dir, manifest, cfg):
meta = []
blocks = []
for layer in 0..cfg.n_layers−1:
(W1, W2) = get_ffn_weights(tensors, layer)
J = alpha_act(cfg) * W2 * W1
(U, S, V, err_max, ok) = compress_J(J, cfg)
if ok:
offset = append_block(blocks, U,S,V)
meta.append({layer, offset, rank=len(S), enabled=1, err_max})
else:
meta.append({layer, offset=0, rank=0, enabled=0, err_max})
write_ffn_arrays(out_dir, blocks, meta)
update_manifest_ffn(manifest, meta, cfg)

12.6 fit_rational_memory

function fit_rational_memory(tensors_or_cfg, out_dir, manifest, cfg):
h = build_positional_impulse(cfg)
(coeff, mode, orders, cos_sim) = find_best_rational(h, cfg)
write_memory_arrays(out_dir, coeff)
update_manifest_memory(manifest, mode, orders, cos_sim, cfg)
	13.	Proof obligations and checks

Before accepting the artifact, STK performs:
	1.	Tokenizer checks
	•	SP cert positive; no epsilon in SP sets.
	•	For random strings, encode-decode round-trip yields identity.
	•	Probe bound per byte <= L_tok.
	2.	PRF checks
	•	Kernel error err_rel <= tol_prf or flagged degraded.
	3.	Linear checks
	•	MPHF injective on K_base.
	•	No slot unused beyond allowed slack (load factor under threshold).
	4.	FFN checks (if enabled)
	•	For each layer, err_max <= tol_ffn or layer disabled.
	5.	Memory checks
	•	Pole magnitudes < 1.
	•	cos_sim >= cos_min or memory disabled.
	6.	Array integrity
	•	Each array header matches expected dims and byte_len.
	•	crc32c and sha256_low over payload recomputed and match header.

Manifest records:
	•	For each module: status enum {OK, DEGRADED, DISABLED}.

Sera may enforce additional runtime constraints based on these statuses.
	14.	Complexity and resource summary

Let:
	•	P_total be the total number of scalar parameters.
	•	V_size be vocabulary size.
	•	r_prf, r_ffn, T_fit_pos be configuration constants.

Then:
	•	Tokenizer:
	•	Time: O(V_size L_tok).
	•	Memory: O(V_size).
	•	PRF:
	•	Time: O(r_prf * d_model + K_test * r_prf * d_model).
	•	Memory: O(r_prf * d_model).
	•	Linear heads:
	•	Time: O(d_model * d_out_total) to scan weights.
	•	MPHF build: O(C_base) with constant-factor overhead.
	•	Memory: O(C_base).
	•	FFN low-rank:
	•	Time per layer: O(d_model * d_ffn * min(d_model, d_ffn)) dominated by truncated SVD; bounded by configuration.
	•	Memory: at most one layer’s FFNs in RAM + scratch.
	•	Rational memory:
	•	Time: O(T_fit_pos * (p_max + q_max + L_max_SOS) * iter_max).
	•	Memory: O(T_fit_pos + model order).

Total peak RAM is:

peak_ram <= max_layer_bytes + scratch_budget

with scratch_budget a constant configured per deployment.

No stage has per-step or per-weight unbounded loops; all search and iteration counters have explicit finite caps.
	15.	Failure and downgrade semantics

STK never writes partially-initialized structures for enabled modules. For each module:
	•	If all checks pass: manifest..status = OK; enable flag = 1.
	•	If kernel/fit error exceeds tolerance but minimal constraints permit a weaker variant:
	•	manifest..status = DEGRADED; enable flag may remain 1; quality metrics included.
	•	If no acceptable variant can be constructed within bounds:
	•	manifest..status = DISABLED; enable flag = 0; arrays are either trivial (zeros, identity) or omitted.

Sera’s loader:
	•	Must treat DISABLED modules as absent and set corresponding runtime paths to no-op or identity.
	•	May treat DEGRADED as OK but adjust bridge or trust policies accordingly.

